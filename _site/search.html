<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@300;400;500;700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/>

    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="/css/animate.css">
    <link rel="stylesheet" type="text/css" href="/css/style.css">
    <link rel="stylesheet" type="text/css" href="/css/responsive.css">

    <link rel="icon" href="images/zasper_logo.ico" type="image/gif" sizes="32x32">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-VRJB8WTXR1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-VRJB8WTXR1');
    </script>

    <title>Prasun's Blog</title>
  </head>
  <body>
    <section id="header-home">
      <nav class="navbar navbar-expand-lg">
        <div class="container">
          <a class="navbar-brand" href="/">
            <!-- <img src="images/logo.svg" alt="#"> -->
            Prasun's blog
          </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvasMenu">
            <i class="fa fa-bars"></i>
          </button>
          <div class="navbar-collapse justify-content-center offcanvas offcanvas-top offcanvas-body" id="offcanvasMenu">
            <button type="button" class="colse-btn" data-bs-dismiss="offcanvas">
              <i class="fa fa-times"></i>
            </button>
            <ul class="navbar-nav">
              <li class="nav-item">
                <a class="nav-link active-menu" href="/about">About</a>
              </li>
            </ul>
            <div class="menu-right-btn mobile-menu-login-signup">
              <a href="https://zasper.io/login" class="custom-btn btn-border hover-bg">Login</a>
              <a href="https://zasper.io/signup" class="custom-btn btn-bg hover-bg">Signup</a>
            </div>
          </div>
          <div class="menu-right-btn justify-content-end">
            <a href="https://zasper.io/login" class="custom-btn btn-border hover-bg">Login</a>
            <a href="https://zasper.io/signup" class="custom-btn btn-bg hover-bg">Signup</a>
          </div>
        </div>
      </nav>
    </section>


    <section id="blog-sec">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-12 col-lg-9">

			<div class="search-wraper wow fadeInUp" data-wow-delay="0.2s">
                <div class="search-form-wraper">
                  <form action="/search.html" method="GET">
                    <input type="text" name="query" id="search-box" placeholder="Search">
                    <button class="btn-bg"><img src="images/icons/search.svg" alt="#"></button>
                  </form>
                </div>
            </div>

			<br>
			<br>
			<div id="search-results"></div>


			<script>
			  window.store = {
			    
			      "cuda-opencl-ruby-rbcuda-2018-02-24-rubygrant17-rbcuda-installation-and-architecture-html": {
			        "title": "RbCUDA: Installation and Architecture",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "This post explains the architecture of RbCUDA and how you can install it on your machine.todo:I have been working on “Port NMatrix to JRuby” as my GSoC project. I am pleased to announce that JRuby is ready for Nmatrix users.NMatrix, a linear algebra library wraps Apache Commons Maths for its core functionalities. By the end of GSoC, I have been able to implement NMatrix for dense matrices with double and object ( ruby objects ) data type. I have also worked on porting mixed-models gem to JRuby which heavily uses NMatrix at its core.InstallationInstall CUDA on your machine.Building RbCUDA from source.git clone https://github.com/prasunanand/rbcudacd rbcudabundle installrake compileInstalling the gemgem build rbcuda.gemspecgem install rbcuda-0.0.0.gemTo check if installation was successful, run pry.$ rake prypry -r './lib/rbcuda.rb'[1] pry(main)&gt; RbCUDA::CUDA.cuInit(0);[2] pry(main)&gt; device = RbCUDA::CUDA.cuDeviceGet(0);[3] pry(main)&gt; puts device#&lt;RbCUDA::RbCuDevice:0x00000001a9a2d0&gt;=&gt; nil[4] pry(main)&gt; puts RbCUDA::CUDA.cuDeviceGetName(100, device);GeForce GTX 750 TiIf you are successfully able to retrive the name of GPU card, you are all set.Code organisationextconf.rb that helps in building the shared object file can be found here.rbcuda.h defines all the Ruby structs that correspond to CUDA types. In the following code CUfunction type can be represented as function_ptr.typedef struct FUNCTION_PTR{  CUfunction function;}function_ptr;typedef struct DEVICE_PTR{  CUdevice device;}device_ptr;The struct fuction_ptr is then wrapped by a Ruby object called RbCuFunction in the file ruby_rbcuda.c.RbCUDA = rb_define_module(\"RbCUDA\");VALUE RbCuDevice = Qnil;VALUE RbCuFunction = Qnil;RbCuDevice    = rb_define_class_under(RbCUDA, \"RbCuDevice\",    rb_cObject);RbCuFunction  = rb_define_class_under(RbCUDA, \"RbCuFunction\",  rb_cObject);Dev ArrayAn array in RbCUDA is handled using Dev_Array class. Implementation is as follows:typedef struct DEV_PTR{  double* carray;}dev_ptr;Dev_Array = rb_define_class_under(RbCUDA, \"Dev_Array\", rb_cObject);A Dev Array stores the pointer to the array data stored on the GPU. The usage will be explained in the next blog.FunctionalitiesRbCUDA has the following modules:  CUDA : It consists of low-level APIs called the CUDA driver APIs.  Runtime : It consists of higher-level APIs called the CUDA runtime APIs that are implemented on top of the CUDA driver APIs.  CuBLAS : It consists of BLAS APIs provided by cuBLAS library.  CuSolver : It consists of APIs provided by cuSolver library.  CuRand : It consists APIs provided by cuRand library.  Profiler : It consists of APIs for profiling CUDA code.ConclusionI have explained how the underlying architecture looks like.We have got RbCUDA successfully installed on our system. In the next blog I will talk about implementing Runtime APIs.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/cuda/opencl/ruby/rbcuda/2018/02/24/rubygrant17-rbcuda-installation-and-architecture.html"
			      }
			      ,
			    
			      "cuda-opencl-ruby-rbcuda-2017-12-30-rubygrant17-rbcuda-introduction-html": {
			        "title": "RbCUDA: CUDA bindings for Ruby",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "I have been working on a new project called RbCUDA.In this project, I want to make it possible to combine thebeauty of Ruby with transparent GPU processing with minimal overheads, so that software developers can easily usethat power when available, and farm out computations transparently to GPU and CPU.Nvidia has been developing a lot of GPU accelerated libraries for linear algebra(CUDA, cuBLAS, cuSolver), Random NumberGeneration(cuRand), fast-fourier transform(cuFFT), Parallel Primitives and Data Structures(Thrust) and image processing(NVIDIA Performance Primitives Library). Many Python/Julia/C++ libraries are being built on top of these libraries.RbCUDA would fill this gap in the Ruby ecosystem making it a highly-preferable option for GPU computing.It will work on Nvidia hardware running on client computers and on servers that make use of TESLA’s.Recently, I was at Nvidia Developer Connect in my city. I was completely amazed by the new hardware launched by Nvidia this year. Itmakes me really confident about the great impact this project will create in the Ruby ecosystem.Previous Effortssgc-ruby-cuda is an existing alternative for CUDA Ruby bindings. However, this library just helps in executing custom kernels.It also doesn’t have support cuBLAS, and other cuMath libraries.ObjectivesThe main objectives of RbCUDA are:  Map all of CUDA into Ruby  Ready-made on-GPU linear algebra, reduction, scan using cuBLAS, cuMath, cuSolver libraries.  Random Numer generator using cuRand  Near-zero wrapping overhead.  CUDA profiler for Ruby.In the near future:  fast-fourier transform(cuFFT)  Parallel Primitives and Data Structures(Thrust)  Image processing (NVIDIA Performance Primitives Library).  Use CuDNN to power a Deep Learning Library written in Ruby.Codebasehttps://github.com/prasunanand/rbcudaContributions are welcome!ExampleMost of the times, when we want to use CUDA for accelerating our code, we end up optimizing matrix-multiplication.I wrote an example that showcases how RbCUDAis the most efficient Ruby library to be used for Number crunching.The example copies two matrices from CPU to GPU, runs matrix multiplication over it. Then the resulting matrix is copiedback to the CPU from GPU.I benchmarked my code and compare it with other existing Ruby libraries.(Note: The above benchmarks have been done on an AMD FX 8350 octacore processor and Nvidia GTX 750Ti GPU.CUDA backend of ArrayFire was used with double floating points.)RbCUDA is the fastest among all. Yay!RbCUDA is the fastest of all the Ruby libraries. The time taken for matrix multiplication is 0.000017s on my machine. The plain C codetakes me 0.000013s for this calculation.RbCUDA is 24x faster than ArrayFire for matrix multiplication, Most of the speed gain going straight to CUDA is probably fromremoving an interaction layer (and buffers) as well as how the data is organized and fed to the underlying architecture.Hence, an overhead of 0.000004s over plain C code makes, it highly efficient Maths library in Ruby.The great thing about this example is that the code is very Rubyish &lt;3.The Code Implementation of matrix multiplication will be discussed in my upcoming blog posts.Profiling matrix multiplication.I have also created a profiler for RbCUDA . The profiling example can be found here.Output of the code.# CUDA_PROFILE_LOG_VERSION 2.0# CUDA_DEVICE 0 GeForce GTX 750 Ti# CUDA_CONTEXT 1# TIMESTAMPFACTOR 14e96c24317d8324method,gputime,cputime,occupancymethod=[ memcpyHtoD ] gputime=[ 16.544 ] cputime=[ 18.705 ]method=[ memcpyHtoD ] gputime=[ 16.288 ] cputime=[ 22.089 ]method=[ memcpyHtoD ] gputime=[ 1.216 ] cputime=[ 6.923 ]method=[ _Z19gemm_kernel2x2_coreIdLb0ELb0ELb0ELb0ELb0EEvPT_PKS0_S3_iiiiiiS1_S1_S0_S0_i ] gputime=[ 76.736 ] cputime=[ 11.967 ] occupancy=[ 0.469 ]method=[ _Z19gemm_kernel2x2_coreIdLb0ELb0ELb0ELb0ELb0EEvPT_PKS0_S3_iiiiiiS1_S1_S0_S0_i ] gputime=[ 73.024 ] cputime=[ 6.485 ] occupancy=[ 0.469 ]method=[ _Z19gemm_kernel2x2_coreIdLb0ELb0ELb0ELb0ELb0EEvPT_PKS0_S3_iiiiiiS1_S1_S0_S0_i ] gputime=[ 72.928 ] cputime=[ 5.739 ] occupancy=[ 0.469 ]method=[ _Z19gemm_kernel2x2_coreIdLb0ELb0ELb0ELb0ELb0EEvPT_PKS0_S3_iiiiiiS1_S1_S0_S0_i ] gputime=[ 72.928 ] cputime=[ 5.725 ] occupancy=[ 0.469 ]method=[ _Z19gemm_kernel2x2_coreIdLb0ELb0ELb0ELb0ELb0EEvPT_PKS0_S3_iiiiiiS1_S1_S0_S0_i ] gputime=[ 72.896 ] cputime=[ 5.701 ] occupancy=[ 0.469 ](Note: The unit of time is microsecond).Ruby Grant 2017This project is funded by Ruby Association. I am very thankful to them for supporting this project.Link.Contributions are welcome!I request all the Rubyists if you have a Ruby code that you want to be accelerated by GPU, please try RbCUDA / ArrayFire. If you areinto Deep learning, let me know if you want to use RbCUDA to power your neural nets for GPU acceleration.Feel free to open an issue!Stay tuned for more posts about GPU computing with Ruby.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/cuda/opencl/ruby/rbcuda/2017/12/30/rubygrant17-rbcuda-introduction.html"
			      }
			      ,
			    
			      "arrayfire-cuda-opencl-ruby-2017-08-28-gsoc17-project-report-html": {
			        "title": "GSoC 2017: Creating the fastest math libraries for Ruby by using the GPU through OpenCL and ArrayFire.",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "GSoC 2017 is about to end. This post summarises my work during the course of summer.todo:I have been working on “Port NMatrix to JRuby” as my GSoC project. I am pleased to announce that JRuby is ready for Nmatrix users.NMatrix, a linear algebra library wraps Apache Commons Maths for its core functionalities. By the end of GSoC, I have been able to implement NMatrix for dense matrices with double and object ( ruby objects ) data type. I have also worked on porting mixed-models gem to JRuby which heavily uses NMatrix at its core.ApplicationThe GSoC 2017 application can be found here.CodeArrayFire-rb: The pull request is undergoing a review.ArrayFire-rb Benchmarks: Codebase can be found here.Bio::FasterLmmD : Codebase can be found hereGoalsArrayFire-rb now supports linear algebra on GPU and CPU. Currently only double dtype has been implemented.It supports dense and sparse matrices. It has multiple backends namely, CUDA, OpenCL and CPU.The work on creating the bindings have been explained in last nine blog posts:  Ruby C extensions for complex projects  Installation  Af_Array(see performance)  Test-suite and Algorithm class  BLAS and LAPACK routines(see performance)  Statistics and Random Engine routines  Device and Util  Multiple Backends: CUDA, OpenCL and CPU  ArrayFire-NMatrix InterfaceThe performance of ArrayFire-rb is outstanding as expected.I took a side-track working on Bio::FasterLmmD . This work is not complete and still in progress.It is an effort to call D from Ruby. The work has been explained in a previous blog post.The work on ArrayFire-rb - JRuby has been postponed for now as I wanted to concentrate on MRI forthe best results.Future WorkThe future work involves improving the ArrayFire-rb code and writing tutorials. ArrayFire is not limited tolinear algebra so I will create bindings for Signal Processing, Computer Vision, etc. I will also add supportfor data types other than double.The work on ArrayFire-rb - JRuby will begin as soon as ArrayFire gem is published.FOSSThis has been my second GSoC with SciRuby. It has been more than an year contibuting extensively to FOSS.It has made me a 1000X better programmer that I used to be.I really appreciate the effort by Google Open Source Committee for conducting GSoC every year. It is thebest platform for the aspiring programmers improve their skill and give back to society by developing freeand open source software.Last year’s GSoC work helped me to present a talk at FOSDEM 2017 and Ruby Conf India 2017.  I got activein the Indian Ruby Community. Recently, I have been invited as a speaker to Ruby World Conference 2017, Matsue, Japanto talk on “GPU computing with Ruby”.I plan to continue contributing to open source, strive for improving my skills, and help new programmerscontribute to FOSS. I would be glad if I could mentor students for upcoming GSoCs.AcknowledgementsI would like to express my sincere gratitude to my mentor Pjotr Prins, for his guidance, patience and support.I have learn a lot from him since my last GSoC and still learning. I couldn’t have hoped for a better mentor.I am grateful to Google and the Ruby Science Foundation for this golden opportunity.I am very thankful to John Woods, Sameer Deshmukh, Alexej Gossmann, Gaurav Tamba and Pradeep Garigipatiwho mentored me through the project.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/cuda/opencl/ruby/2017/08/28/gsoc17-project-report.html"
			      }
			      ,
			    
			      "arrayfire-2017-08-24-gsoc17-arrayfire-ruby-bindings-part-8-nmatrix-interface-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part VIII : Interfacing to NMatrix)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "ArrayFire-rb has been interfaced to NMatrix so that they can work together. Sometimes, copying datafrom GPU memory to CPU memory and vice versa can be slower than the computation time. Hence, a lot ofdevelopers prefer to use GPU computing for the most time consuming task. The interface between ArrayFire-rband NMatrix helps in providing maximum freedom to the developers to use the libraries of their choice.This post explains how NMatrix and ArrayFire-rb interface has been created. It starts with addingnmatrix as a dependency to arrayfire.Gemspec  gem.add_development_dependency 'nmatrix', '~&gt; 0.2.1'extconf.rbNext add nmatrix.h header file allowing us to call nmatrix’s C/C++ methods from arrayfire.nmatrix_path = Gem::Specification.find_all_by_name('nmatrix').compactabort \"Cannot locate NMatrix installation\" unless nmatrix_pathnmatrix_header_dir = File.join(nmatrix_path[0].require_path)HEADER_DIRS = [  '/opt/local/include',  '/usr/local/include',  INCLUDEDIR,  '/usr/include',  nmatrix_header_dir]LIB_DIRS = [  '/opt/local/lib',  '/usr/local/lib',  LIBDIR,  '/usr/lib',  nmatrix_header_dir]dir_config(extension_name, HEADER_DIRS, LIB_DIRS)have_library('af')NMatrix C++ APIAn NMatrix Ruby C object is denoted by cNMatrix. Since, cNMatrix has been initially defined in nmatrix.h filewe represent it as extern VALUE and C++ has been specified as we are using the C++interface.Next, we create the Ruby bindings.extern \"C++\" {  VALUE cNMatrix;}void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Af_Array = rb_define_class_under(ArrayFire, \"Af_Array\", rb_cObject);  rb_define_method(Af_Array, \"to_nmatrix\", (METHOD)arf_af_array_to_nmatrix, 0);  cNMatrix = rb_define_class(\"NMatrix\", rb_cObject);  rb_define_method(cNMatrix, \"to_af_array\", (METHOD)arf_nmatrix_to_af_array_method, 0);}The C functions have been implemented as the following code snippets.nm namespace is used to store NMatrix types and structs.Af_Array to NMatrixAn Af_Array#to_nmatrix is called to convert an Af_Array object to NMatrix object. The #to_nmatrix method callsarf_af_array_to_nmatrix method which is responsible for calling NMatrix C++ APIs.The rb_nmatrix_dense_create() is NMatrix API to create a dense NMatrix object. nm::FLOAT64 is the dtype of NMatrix.// ext/mri/interface/nmatrix.cstatic VALUE arf_af_array_to_nmatrix(VALUE self) {  afstruct* input;  Data_Get_Struct(self, afstruct, input);  dim_t count;  uint ndims;  af_get_numdims(&amp;ndims, input-&gt;carray);  dim_t* dims = (dim_t*)malloc(ndims * sizeof(dim_t));  af_get_dims(&amp;dims[0], &amp;dims[1], &amp;dims[2], &amp;dims[3], input-&gt;carray);  size_t* shape = (size_t*)malloc(ndims * sizeof(size_t));;  for (dim_t index = 0; index &lt; ndims; index++){    shape[index] = (size_t)(dims[index]);  }  af_get_elements(&amp;count, input-&gt;carray);  double* elements = (double*)malloc(count * sizeof(double));  af_get_data_ptr(elements, input-&gt;carray);  return rb_nmatrix_dense_create(nm::FLOAT64, shape, ndims, elements, (int)count);}NMatrix to Af_ArrayAn NMatrix#to_af_array is called to convert an NMatrix object to Af_Array object. The #to_af_array method callsarf_nmatrix_to_af_array_method method which is responsible for calling NMatrix C++ APIs. NM_DTYPE() is used  to checkthe dtype of an NMatrix object. Currentlt, only nm::FLOAT64 is supported.The arf_nmatrix_to_af_array is called by arf_nmatrix_to_af_array_method method and an Af_Array object.// ext/mri/interface/nmatrix.cextern VALUE arf_nmatrix_to_af_array_method(VALUE nmatrix) {  if (NM_DIM(nmatrix) &gt; 4) {    rb_raise(rb_eStandardError,      \"NMatrix must not have greater than 4 dimensions.\");  }  if (NM_DTYPE(nmatrix) == nm::FLOAT64) {    return Data_Wrap_Struct(Af_Array, NULL, arf_free, arf_nmatrix_to_af_array(nmatrix));  }  else {    rb_raise(rb_eStandardError,      \"NMatrix should be either :complex64, :complex128, :int32 or :float64 type.\");  }  return Qnil;}afstruct* arf_nmatrix_to_af_array(VALUE nm) {  DENSE_STORAGE* nmat = NM_STORAGE_DENSE(nm);  afstruct* output = ALLOC(afstruct);  if (nmat-&gt;dtype != nm::FLOAT64) {    rb_raise(rb_eStandardError, \"requires dtype of :float64 to convert to an Af_Array\");  }  dim_t* shape = (dim_t*)malloc(nmat-&gt;dim * sizeof(dim_t));;  for (size_t index = 0; index &lt; nmat-&gt;dim; index++){    shape[index] = (size_t)(nmat-&gt;shape[index]);  }  af_create_array(&amp;output-&gt;carray, nmat-&gt;elements, nmat-&gt;dim, shape, f64);  return output;}Let’s  pry it out.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; require 'nmatrix'=&gt; true[2] pry(main)&gt; a = ArrayFire::Af_Array.new 2, [4,4], [1, 2, 2, 0,  -2, 2 , 1, 3, 1, 4, 3 , 1, 0, -3, 2, 9]No Name Array[4 4 1 1]    1.0000    -2.0000     1.0000     0.0000    2.0000     2.0000     4.0000    -3.0000    2.0000     1.0000     3.0000     2.0000    0.0000     3.0000     1.0000     9.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000019bd6f0&gt;[3] pry(main)&gt; b = a.to_nmatrix=&gt;[  [ 1.0,  2.0, 2.0, 0.0]  [-2.0,  2.0, 1.0, 3.0]  [ 1.0,  4.0, 3.0, 1.0]  [ 0.0, -3.0, 2.0, 9.0]][4] pry(main)&gt; c = b.to_af_array=&gt; #&lt;ArrayFire::Af_Array:0x00000001a4d340&gt;[5] pry(main)&gt; c.elements=&gt; [1.0, 2.0, 2.0, 0.0, -2.0, 2.0, 1.0, 3.0, 1.0, 4.0, 3.0, 1.0, 0.0, -3.0, 2.0, 9.0][6] pry(main)&gt; ArrayFire::Util.print_array(c)No Name Array[4 4 1 1]    1.0000    -2.0000     1.0000     0.0000    2.0000     2.0000     4.0000    -3.0000    2.0000     1.0000     3.0000     2.0000    0.0000     3.0000     1.0000     9.0000=&gt; true[7] pry(main)&gt; ArrayFire::Util.print_array(c)No Name Array[4 4 1 1]    1.0000    -2.0000     1.0000     0.0000    2.0000     2.0000     4.0000    -3.0000    2.0000     1.0000     3.0000     2.0000    0.0000     3.0000     1.0000     9.0000=&gt; true[8] pry(main)&gt; c = b.to_af_array=&gt; #&lt;ArrayFire::Af_Array:0x00000001436088&gt;It works!.Hence, NMatrix and ArrayFire can be easily interfaced to each other.BenchmarksNow, we can see how using ArrayFire can beneficial over using NMatrix. I benchmarked the matrix multiplication fortwo matrices with same elements using code.shapeArray.each do |shape|  elements1 = Array.new(shape[0]*shape[1]) { rand(1...999999) }  elements2 = Array.new(shape[0]*shape[1]) { rand(1...999999) }  cpu_matrix1 = NMatrix.new(shape, elements1, dtype: :float64)  cpu_matrix2 = NMatrix.new(shape, elements2, dtype: :float64)  gpu_matrix1 = cpu_matrix1.to_af_array  gpu_matrix2 = cpu_matrix2.to_af_array  iters.times do    ArrayFire::BLAS.matmul(gpu_matrix1, gpu_matrix2, :AF_MAT_NONE, :AF_MAT_NONE) # warmup  end  result[:mat_mult_cpu] &lt;&lt; [ shape[0]*shape[1], Benchmark.measure{cpu_matrix1.dot(cpu_matrix2)}.to_s.tr('()', '').split(\" \")[3].to_f ]  result[:mat_mult_gpu] &lt;&lt; [ shape[0]*shape[1], Benchmark.measure{ArrayFire::BLAS.matmul(gpu_matrix1, gpu_matrix2, :AF_MAT_NONE, :AF_MAT_NONE)}.to_s.tr('()', '').split(\" \")[3].to_f ]end                              (Note: The above benchmarks have been done on an AMD FX 8350 octacore processorand Nvidia GTX 750Ti GPU. CUDA backend of ArrayFire was used with double floating points.)As the matrix size increases, we can see the difference is huge.Using ArrayFire can speed up the calculation by 7 e +5 times.There may be overheads in copying data from CPU to GPU and vice-versa. But overall,for large matrices(Big Data), we can gain massive speedups when we use the right optimization.I would be discussing more about such optimizations in another blog post.However, ArrayFire being significantly faster than NMatrix can easily help Rubyists by adding smallchunk of code and speeding up the critical/slower steps.Power to Ruby!Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/08/24/gsoc17-arrayfire-ruby-bindings-part-8-nmatrix-interface.html"
			      }
			      ,
			    
			      "arrayfire-2017-08-24-gsoc17-arrayfire-ruby-bindings-part-7-backend-cuda-and-opencl-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part VII : Multiple Backends: CUDA, OpenCL and CPU)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "todo:I have been working on “Port NMatrix to JRuby” as my GSoC project. I am pleased to announce that JRuby is ready for Nmatrix users.NMatrix, a linear algebra library wraps Apache Commons Maths for its core functionalities. By the end of GSoC, I have been able to implement NMatrix for dense matrices with double and object ( ruby objects ) data type. I have also worked on porting mixed-models gem to JRuby which heavily uses NMatrix at its core.ArrayFire can run not just on GPU devices but also on CPU devices, which is one of its really cool features.With version 3.2 the unified-backend helps in changing the ArrayFire backend on the fly.When using the unified-backend, the preference order for the default backend is CUDA &gt; OpenCL &gt; CPU.The CPU backend requires Intel MKL libraries be installed. To install Intel MKL library, go to the officialdownload link. Next extract the file and run sudo install.sh from theproject root directory.For an AMD CPU, you need to specify the location of libmkl_rt.so.To solve this on a debian machine:$ cd /etc/ld.so.conf.d/$ sudo nano mylibs.confPaste line:/opt/intel/compilers_and_libraries_2017/linux/mkl/lib/intel64_lin/Next, run$ sudo ldconfigNow download arrayfire-rb from link$ git clone https://github.com/prasunanand/arrayfire-rb$ rake compile$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; ArrayFire::Backend.get_backend_count=&gt; 3[2] pry(main)&gt; ArrayFire::Backend.get_active_backend=&gt; \"AF_BACKEND_CUDA\"[3] pry(main)&gt; ArrayFire::Backend.set_backend(:AF_BACKEND_CPU)=&gt; nil[4] pry(main)&gt; ArrayFire::Backend.get_active_backend=&gt; \"AF_BACKEND_CPU\"Voila! You can run ArrayFire on CPU.Lets look into the implementation and what else can we do.BackendThe class structure of Backend class is as follows:void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Backend = rb_define_class_under(ArrayFire, \"Backend\", rb_cObject);  rb_define_singleton_method(Backend, \"get_backend_count\", (METHOD)arf_get_backend_count, 0);  rb_define_singleton_method(Backend, \"get_available_backends\", (METHOD)arf_get_available_backends, 0);  rb_define_singleton_method(Backend, \"get_backend_id\", (METHOD)arf_get_backend_id, 1);  rb_define_singleton_method(Backend, \"get_active_backend\", (METHOD)arf_get_active_backend, 0);  rb_define_singleton_method(Backend, \"get_device_id\", (METHOD)arf_get_backend_device_id, 1);  rb_define_singleton_method(Backend, \"set_backend\", (METHOD)arf_set_backend, 1);}The C bindings were implemented as:static VALUE arf_get_backend_count(VALUE self){  uint num_backends;  af_get_backend_count(&amp;num_backends);  return UINT2NUM(num_backends);}static VALUE arf_get_available_backends(VALUE self){  int backends;  af_get_available_backends(&amp;backends);  return INT2NUM(backends);}static VALUE arf_get_backend_id(VALUE self, VALUE array_val){  afstruct* input;  Data_Get_Struct(array_val, afstruct, input);  af_backend backend;  af_get_backend_id (&amp;backend, input-&gt;carray);  const char* backend_name = get_backend_name(backend);  return rb_str_new_cstr(backend_name);}static VALUE arf_get_active_backend(VALUE self){  af_backend backend;  af_get_active_backend(&amp;backend);  const char* backend_name = get_backend_name(backend);  return rb_str_new_cstr(backend_name);}static VALUE arf_set_backend(VALUE self, VALUE backend_val){  af_backend backend = arf_backend_type_from_rbsymbol(backend_val);  af_set_backend(backend);  return Qnil;}An af_backend  symbol can be :AF_BACKEND_DEFAULT, :AF_BACKEND_CPU, :AF_BACKEND_CUDA, or :AF_BACKEND_OPENCL.The implemetation is given below.std::map&lt;char*, size_t&gt; BACKEND_TYPES = {  {\"AF_BACKEND_DEFAULT\" , 0},                        ///&lt; Default backend order: OpenCL -&gt; CUDA -&gt; CPU  {\"AF_BACKEND_CPU\"     , 1},                        ///&lt; CPU a.k.a sequential algorithms  {\"AF_BACKEND_CUDA\"    , 2},                        ///&lt; CUDA Compute Backend  {\"AF_BACKEND_OPENCL\"  , 4}                         ///&lt; OpenCL Compute Backend};af_backend arf_backend_type_from_rbsymbol(VALUE sym) {  ID sym_id = SYM2ID(sym);  for(std::map&lt;char*, size_t&gt;::value_type&amp; entry : BACKEND_TYPES) {    if (sym_id == rb_intern(entry.first)) {      return static_cast&lt;af_backend&gt;(entry.second);    }  }  VALUE str = rb_any_to_s(sym);  rb_raise(rb_eArgError, \"invalid backend type symbol (:%s) specified\", RSTRING_PTR(str));}ArrayFire::Backend.get_available_backends lets you know the backends available.This is the table that helps you decipher the backends available.            Value      Backends                  0      None              1      CPU              2      CUDA              3      CPU and CUDA              4      OpenCL              5      CPU and OpenCL              6      CUDA and OpenCL              7      CPU, CUDA and OpenCL      Using rake pry to investigate the backend and device available for computing:$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; backends = ArrayFire::Backend.get_available_backends=&gt; 7[2] pry(main)&gt; ArrayFire::Backend.get_backend_count=&gt; 3[3] pry(main)&gt; ArrayFire::Backend.get_active_backend=&gt; \"AF_BACKEND_CUDA\"[4] pry(main)&gt; ArrayFire::Device.infoArrayFire v3.4.0 (CUDA, 64-bit Linux, build 75cad40)Platform: CUDA Toolkit 7.5, Driver: 375.66[0] GeForce GTX 750 Ti, 4042 MB, CUDA Compute 5.0=&gt; nil[5] pry(main)&gt; ArrayFire::Backend.set_backend(:AF_BACKEND_CPU)=&gt; nil[6] pry(main)&gt; ArrayFire::Device.infoArrayFire v3.4.0 (CPU, 64-bit Linux, build 75cad40)[0] AMD: AMD FX(tm)-8350 Eight-Core Processor           , 16077 MB, Max threads(8)=&gt; nil[7] pry(main)&gt; ArrayFire::Backend.set_backend(:AF_BACKEND_OPENCL)=&gt; nil[8] pry(main)&gt; ArrayFire::Device.infoArrayFire v3.4.0 (OpenCL, 64-bit Linux, build 75cad40)[0] NVIDIA  : GeForce GTX 750 Ti, 4041 MBArrayFire also helps in interacting custom OpenCL and CUDA kernels. Since, Ruby currentlydoesn’t have rbCUDA and rbOpenCL gems (Ruby bindings for  CUDA and OpenCL respectively), wecan’t do much with ArrayFire-rb.ConclusionArrayFire-rb can be used as a standalone linear algebra library not restricted to GPUs.A user can change the backend on fly making it easier to leverage the power of CUDA and OpenCL. Theorder of preference of default backend to harness maximum computing power helps himnot worry too much about performance tuning in the early stages of development, hence supportingthe philosophy of preferring convention over configuration.In the next blog post, I will discuss about interfacing ArrayFire-rb to NMatrix.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/08/24/gsoc17-arrayfire-ruby-bindings-part-7-backend-cuda-and-opencl.html"
			      }
			      ,
			    
			      "arrayfire-2017-08-22-gsoc17-arrayfire-ruby-bindings-part-6-device-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part VI : Device and Util)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "ArrayFire has Device and Util class that contains of methods which can be used to manage the CPU and GPUdevices and various utility functions like printing and storing the arrays.This blog will explain my work regarding the bindings I have created and how to use them.DeviceDevice class contains of singleton methods like info, device_gc, lock_device_ptr that canbe used to get info about the GPU devices used for calculation, explicitly call the garbage collectorand locking the pointers on GPU device.The implementation is as follows:void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Device = rb_define_class_under(ArrayFire, \"Device\", rb_cObject);  rb_define_singleton_method(Device, \"info\", (METHOD)arf_info, 0);  rb_define_singleton_method(Device, \"init\", (METHOD)arf_init2, 0);  rb_define_singleton_method(Device, \"info_string\", (METHOD)arf_info_string, 0);  rb_define_singleton_method(Device, \"device_info\", (METHOD)arf_device_info, 0);  rb_define_singleton_method(Device, \"get_device_count\", (METHOD)arf_get_device_count, 0);  rb_define_singleton_method(Device, \"get_dbl_support\", (METHOD)arf_get_dbl_support, 1);  rb_define_singleton_method(Device, \"set_device\", (METHOD)arf_set_device, 1);  rb_define_singleton_method(Device, \"get_device\", (METHOD)arf_get_device, 0);  rb_define_singleton_method(Device, \"sync\", (METHOD)arf_sync, 1);  rb_define_singleton_method(Device, \"device_mem_info\", (METHOD)arf_device_mem_info, 0);  rb_define_singleton_method(Device, \"print_mem_info\", (METHOD)arf_print_mem_info, 2);  rb_define_singleton_method(Device, \"device_gc\", (METHOD)arf_device_gc, 0);  rb_define_singleton_method(Device, \"set_mem_step_size\", (METHOD)arf_set_mem_step_size, 1);  rb_define_singleton_method(Device, \"get_mem_step_size\", (METHOD)arf_get_mem_step_size, 0);  rb_define_singleton_method(Device, \"lock_device_ptr\", (METHOD)arf_lock_device_ptr, 1);  rb_define_singleton_method(Device, \"unlock_device_ptr\", (METHOD)arf_unlock_device_ptr, 1);  rb_define_singleton_method(Device, \"lock_array\", (METHOD)arf_lock_array, 1);  rb_define_singleton_method(Device, \"unlock_array\", (METHOD)arf_unlock_array, 1);  rb_define_singleton_method(Device, \"is_locked_array\", (METHOD)arf_is_locked_array, 1);  rb_define_singleton_method(Device, \"get_device_ptr\", (METHOD)arf_get_device_ptr, 0);}The implementation of device_info is interesting as it has 4 return values in the form of strings.ArrayFire::Device.device_info passes the variables to be returned to  ArrayFire::Device.device_info_funcwhich modifies the contents of the variables passed.module ArrayFire  class Device    def self.device_info      d_name     = \"\"      d_platform = \"\"      d_toolkit  = \"\"      d_compute  = \"\"      ArrayFire::Device.device_info_func(d_name, d_platform, d_toolkit, d_compute);      return d_name, d_platform, d_toolkit, d_compute    end  endendThe point to note here is that ArrayFire::Device.device_info uses rb_str_cat2() to concatenate the char*to the VALUE containing empty string.static VALUE arf_device_info(VALUE self, VALUE name_val, VALUE platform_val, VALUE toolkit_val, VALUE compute_val){  char* d_name     = (char*)malloc(sizeof(char) * 64);  char* d_platform = (char*)malloc(sizeof(char) * 10);  char* d_toolkit  = (char*)malloc(sizeof(char) * 64);  char* d_compute  = (char*)malloc(sizeof(char) * 10);  af_device_info(d_name, d_platform, d_toolkit, d_compute);  rb_str_cat2(name_val,     d_name);  rb_str_cat2(platform_val, d_platform);  rb_str_cat2(toolkit_val,  d_toolkit);  rb_str_cat2(compute_val,  d_compute);  return Qnil;}static VALUE arf_device_mem_info(VALUE self){  size_t alloc_bytes, alloc_buffers, lock_bytes, lock_buffers;  af_device_mem_info( &amp;alloc_bytes, &amp;alloc_buffers, &amp;lock_bytes, &amp;lock_buffers);  printf(\"Allocated Bytes: %d\\nAllocated buffers: %d\\nLock Bytes: %d\\nLock Buffers: %d\\n\",          alloc_bytes, alloc_buffers, lock_bytes, lock_buffers);  return Qnil;}Running pry to check the bindings:rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; ArrayFire::Device.infoArrayFire v3.4.0 (CUDA, 64-bit Linux, build 75cad40)Platform: CUDA Toolkit 7.5, Driver: 375.66[0] GeForce GTX 750 Ti, 4042 MB, CUDA Compute 5.0=&gt; nil[2] pry(main)&gt; ArrayFire::Device.device_info=&gt; [\"GeForce_GTX_750_Ti\", \"CUDA\", \"v7.5\", \"5.0\"][3] pry(main)&gt; ArrayFire::Device.get_device_count=&gt; 1[4] pry(main)&gt; ArrayFire::Device.get_dbl_support(0)=&gt; true[5] pry(main)&gt; ArrayFire::Device.device_mem_infoAllocated Bytes: 0Allocated buffers: 0Lock Bytes: 0Lock Buffers: 0=&gt; nil[6] pry(main)&gt; arr = ArrayFire::Af_Array.new 2, [4,4], [1, 2, 2, 0, -2, 2 , 1, 3, 1, 4, 3 , 1, 0, -3, 2, 9]No Name Array[4 4 1 1]    1.0000    -2.0000     1.0000     0.0000    2.0000     2.0000     4.0000    -3.0000    2.0000     1.0000     3.0000     2.0000    0.0000     3.0000     1.0000     9.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000023800c0&gt;[7] pry(main)&gt; ArrayFire::Device.print_mem_info(\"mem info\", 0)mem info---------------------------------------------------------|     POINTER      |    SIZE    |  AF LOCK  | USER LOCK |---------------------------------------------------------|     0x702f80000  |       1 KB |       Yes |        No ||     0x702f80400  |       1 KB |        No |        No |---------------------------------------------------------=&gt; nil[8] pry(main)&gt; ArrayFire::Device.device_mem_infoAllocated Bytes: 2048Allocated buffers: 2Lock Bytes: 1024Lock Buffers: 1=&gt; nil[9] pry(main)&gt; ArrayFire::Device.get_mem_step_size=&gt; 1024[10] pry(main)&gt; ArrayFire::Device.is_locked_array(arr)=&gt; false[11] pry(main)&gt; ArrayFire::Device.lock_array(arr)=&gt; true[12] pry(main)&gt; ArrayFire::Device.is_locked_array(arr)=&gt; trueUtilUtil class contains of methods responsible for printing an Af_Array, saving it in a fileor converting it to string and other utility methods.The implementation is as follows:void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Util = rb_define_class_under(ArrayFire, \"Util\", rb_cObject);  rb_define_singleton_method(Util, \"print_array\", (METHOD)arf_print_array, 1);  rb_define_singleton_method(Util, \"print_array_gen\", (METHOD)arf_print_array_gen, 0);  rb_define_singleton_method(Util, \"save_array\", (METHOD)arf_save_array, 4);  rb_define_singleton_method(Util, \"read_array_index\", (METHOD)arf_read_array_index, 0);  rb_define_singleton_method(Util, \"read_array_key\", (METHOD)arf_read_array_key, 0);  rb_define_singleton_method(Util, \"read_array_key_check\", (METHOD)arf_read_array_key_check, 0);  rb_define_singleton_method(Util, \"array_to_string\", (METHOD)arf_array_to_string, 4);  rb_define_singleton_method(Util, \"get_version\", (METHOD)arf_get_version, 0);  rb_define_singleton_method(Util, \"get_revision\", (METHOD)arf_get_revision, 0);  rb_define_singleton_method(Util, \"get_size_of\", (METHOD)arf_get_size_of, 1);}af_array_to_string API has also been used to implement ArrayFire::Af_Array#to_s.The implementation of arf_get_version is interesting as it returns a hash value that representsthe version of ArrayFire C library installed using the keys as major, minor and patch.rb_hash_new and rb_hash_aset has been used to create a hash.static VALUE arf_print_array(VALUE self, VALUE input_val){  afstruct* input;  Data_Get_Struct(input_val, afstruct, input);  af_print_array(input-&gt;carray);  return Qtrue;}static VALUE arf_save_array(VALUE self, VALUE key_val, VALUE array_val, VALUE fn_val, VALUE append){  afstruct* input;  Data_Get_Struct(array_val, afstruct, input);  const char* key = StringValueCStr(key_val);  const char* filename = StringValueCStr(fn_val);  int index;  af_save_array (&amp;index, key, input-&gt;carray, filename, RTEST(append));  return INT2NUM(index);}static VALUE arf_array_to_string(VALUE self, VALUE exp_val, VALUE array_val, VALUE precision, VALUE transpose){  char* output;  afstruct* input;  Data_Get_Struct(array_val, afstruct, input);  const char* exp = StringValueCStr(exp_val);  af_array_to_string(&amp;output, exp, input-&gt;carray, NUM2INT(precision), RTEST(transpose));  return rb_str_new_cstr(output);}static VALUE arf_get_version(VALUE self){  int major, minor, patch;  af_get_version(&amp;major, &amp;minor, &amp;patch);  VALUE hash = rb_hash_new();  rb_hash_aset(hash, rb_str_new_cstr(\"major\"), INT2NUM(major));  rb_hash_aset(hash, rb_str_new_cstr(\"minor\"), INT2NUM(minor));  rb_hash_aset(hash, rb_str_new_cstr(\"patch\"), INT2NUM(patch));  return hash;}Lets use pry to check the bindings:$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; arr = ArrayFire::Af_Array.new 2, [4,4], [1, 2, 2, 0,  -2, 2 , 1, 3, 1, 4, 3 , 1, 0, -3, 2, 9]No Name Array[4 4 1 1]    1.0000    -2.0000     1.0000     0.0000    2.0000     2.0000     4.0000    -3.0000    2.0000     1.0000     3.0000     2.0000    0.0000     3.0000     1.0000     9.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000026bde18&gt;[2] pry(main)&gt; ArrayFire::Util.print_array(arr)No Name Array[4 4 1 1]    1.0000    -2.0000     1.0000     0.0000    2.0000     2.0000     4.0000    -3.0000    2.0000     1.0000     3.0000     2.0000    0.0000     3.0000     1.0000     9.0000=&gt; true[3] pry(main)&gt; x = ArrayFire::Util.array_to_string(\"GPU Array\", arr, 5, false)=&gt; \"GPU Array\\n[4 4 1 1]\\n    1.00000     2.00000     2.00000     0.00000 \\n   -2.00000     2.00000     1.00000     3.00000 \\n    1.00000     4.00000     3.00000     1.00000 \\n    0.00000    -3.00000     2.00000     9.00000 \\n\\n\"[4] pry(main)&gt; puts xGPU Array[4 4 1 1]    1.00000     2.00000     2.00000     0.00000   -2.00000     2.00000     1.00000     3.00000    1.00000     4.00000     3.00000     1.00000    0.00000    -3.00000     2.00000     9.00000=&gt; nil[5] pry(main)&gt; ArrayFire::Util.get_version=&gt; {\"major\"=&gt;3, \"minor\"=&gt;4, \"patch\"=&gt;0}[6] pry(main)&gt; ArrayFire::Util.get_revision=&gt; \"75cad40\"[7] pry(main)&gt; ArrayFire::Util.get_size_of(:f64)=&gt; 8The bindings work!.ConclusionWe can easily manage the available GPU devices and manage the memory. Also, we can save the Af_Arrayoutput to a file among other utilities.In the next blog post, I will explain about using multiple backends using ArrayFire , i.e. OpenCL, CUDAand even use CPUs.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/08/22/gsoc17-arrayfire-ruby-bindings-part-6-device.html"
			      }
			      ,
			    
			      "arrayfire-2017-08-17-gsoc17-arrayfire-ruby-bindings-part-4-statistics-and-random-engine-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part V : Statistics and Random Engine routines)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "This post explains about the Statistics and Random class of arrayfire gem.ArrayFire::Statistics class consists of  methods that can be used to operate on Af_Array for statistical analysis. ArrayFire::Random class contains of methods to generate Random numbers and array of Random numberson the GPU.Statistics ClassThe Statistics class contains of singleton methods like mean, var, median, stddev, etc.Let us take a look at the implementation.void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Statistics = rb_define_class_under(ArrayFire, \"Statistics\", rb_cObject);  rb_define_singleton_method(Statistics, \"mean\", (METHOD)arf_mean, 2);  rb_define_singleton_method(Statistics, \"mean_weighted\", (METHOD)arf_mean_weighted, 3);  rb_define_singleton_method(Statistics, \"var\", (METHOD)arf_var, 3);  rb_define_singleton_method(Statistics, \"var_weighted\", (METHOD)arf_var_weighted, 3);  rb_define_singleton_method(Statistics, \"stdev\", (METHOD)arf_stdev, 2);  rb_define_singleton_method(Statistics, \"cov\", (METHOD)arf_cov, 3);  rb_define_singleton_method(Statistics, \"median\", (METHOD)arf_median, 2);  rb_define_singleton_method(Statistics, \"mean_all\", (METHOD)arf_mean_all, 1);  rb_define_singleton_method(Statistics, \"mean_all_weighted\", (METHOD)arf_mean_all_weighted, 2);  rb_define_singleton_method(Statistics, \"var_all\", (METHOD)arf_var_all, 2);  rb_define_singleton_method(Statistics, \"var_all_weighted\", (METHOD)arf_var_all_weighted, 2);  rb_define_singleton_method(Statistics, \"stdev_all\", (METHOD)arf_stdev_all, 1);  rb_define_singleton_method(Statistics, \"median_all\", (METHOD)arf_median_all, 1);  rb_define_singleton_method(Statistics, \"corrcoef\", (METHOD)arf_corrcoef, 2);}ArrayFire provides four different methods to calculate the means namely,mean, mean_weighted, mean_all, mean_all_weighted. The implementationis given below.mean and mean_weighted calculate the mean and weighted mean along the dimnensionspecified. However, mean_all and mean_weighted_all calculate the mean and weightedmean over all the elements in an array.static VALUE arf_mean(VALUE self, VALUE array_val, VALUE dim_val){  afstruct* input;  afstruct* output = ALLOC(afstruct);  Data_Get_Struct(array_val, afstruct, input);  af_mean(&amp;output-&gt;carray, input-&gt;carray, NUM2UINT(dim_val));  af_print_array(output-&gt;carray);  return Data_Wrap_Struct(Af_Array, NULL, arf_free, output);}static VALUE arf_mean_weighted(VALUE self, VALUE array_val, VALUE weighted_array_val, VALUE dim_val){  afstruct* input;  afstruct* weighted_array;  afstruct* output = ALLOC(afstruct);  Data_Get_Struct(array_val, afstruct, input);  Data_Get_Struct(weighted_array_val, afstruct, weighted_array);  af_mean_weighted(&amp;output-&gt;carray, input-&gt;carray, weighted_array-&gt;carray, NUM2UINT(dim_val));  af_print_array(output-&gt;carray);  return Data_Wrap_Struct(Af_Array, NULL, arf_free, output);}static VALUE arf_mean_all(VALUE self, VALUE array_val){  afstruct* input;  double real_part, imag_part;  Data_Get_Struct(array_val, afstruct, input);  af_mean_all(&amp;real_part, &amp;imag_part, input-&gt;carray);  return DBL2NUM(real_part);}static VALUE arf_mean_all_weighted(VALUE self, VALUE array_val, VALUE weighted_array_val){  afstruct* input;  afstruct* weighted_array;  double real_part, imag_part;  Data_Get_Struct(array_val, afstruct, input);  Data_Get_Struct(weighted_array_val, afstruct, weighted_array);  af_mean_all_weighted(&amp;real_part, &amp;imag_part, input-&gt;carray, weighted_array-&gt;carray);  return DBL2NUM(real_part);}I check the bindings using pry.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; arr = ArrayFire::Random.randu(2, [4, 4])=&gt; #&lt;ArrayFire::Af_Array:0x0000000308fb08&gt;[2] pry(main)&gt; ArrayFire::Util.print_array(arr)No Name Array[4 4 1 1]    0.3990     0.7353     0.9455     0.7089    0.6720     0.5160     0.1587     0.9434    0.5339     0.3932     0.8831     0.1227    0.1386     0.2706     0.0621     0.9107=&gt; true[3] pry(main)&gt; weighted_array = ArrayFire::Af_Array.new 2, [4,4], [1, 2, 2, 0,  -2, 2 , 1, 3, 1, 4, 3 , 1, 0, -3, 2, 9]No Name Array[4 4 1 1]    1.0000    -2.0000     1.0000     0.0000    2.0000     2.0000     4.0000    -3.0000    2.0000     1.0000     3.0000     2.0000    0.0000     3.0000     1.0000     9.0000=&gt; #&lt;ArrayFire::Af_Array:0x0000000316c008&gt;[4] pry(main)&gt; mean = ArrayFire::Statistics.mean(arr, 1)No Name Array[4 1 1 1]    0.6972    0.5725    0.4832    0.3455=&gt; #&lt;ArrayFire::Af_Array:0x00000003212138&gt;[5] pry(main)&gt; mean_weighted = ArrayFire::Statistics.mean_weighted(arr, weighted_array, 2)No Name Array[4 4 1 1]    0.3990     0.7353     0.9455        nan    0.6720     0.5160     0.1587     0.9434    0.5339     0.3932     0.8831     0.1227       nan     0.2706     0.0621     0.9107=&gt; #&lt;ArrayFire::Af_Array:0x000000031d2588&gt;[6] pry(main)&gt; mean_all = ArrayFire::Statistics.mean_all(arr)=&gt; 0.5246008634567261[7] pry(main)&gt; mean_all_weighted = ArrayFire::Statistics.mean_all_weighted(arr, weighted_array)=&gt; 0.5184718370437622Random ClassArrayFire supports three types of random engines: :AF_RANDOM_ENGINE_PHILOX_4X32_10,:AF_RANDOM_ENGINE_THREEFRY_2X32_16 and :AF_RANDOM_ENGINE_MERSENNE_GP11213 which can bepassed as a type.The default randon engine is :AF_RANDOM_ENGINE_PHILOX_4X32_10. Random class helps  in creatingan engine by specifying the type and seed. A programmer can set the seed on the fly and also createarrays with randomly generated values.void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Random = rb_define_class_under(ArrayFire, \"Random\", rb_cObject);  rb_define_alloc_func(Random, arf_engine_alloc);  rb_define_singleton_method(Random, \"create_random_engine\", (METHOD)arf_create_random_engine, 2);  rb_define_singleton_method(Random, \"retain_random_engine\", (METHOD)arf_retain_random_engine, 1);  rb_define_singleton_method(Random, \"random_engine_set_type\", (METHOD)arf_random_engine_set_type, 0);  rb_define_singleton_method(Random, \"random_engine_get_type\", (METHOD)arf_random_engine_get_type, 0);  rb_define_singleton_method(Random, \"random_uniform\", (METHOD)arf_random_uniform, 3);  rb_define_singleton_method(Random, \"random_normal\", (METHOD)arf_random_normal, 3);  rb_define_singleton_method(Random, \"random_engine_set_seed\", (METHOD)arf_random_engine_set_seed, 2);  rb_define_singleton_method(Random, \"get_default_random_engine\", (METHOD)arf_get_default_random_engine, 0);  rb_define_singleton_method(Random, \"set_default_random_engine_type\", (METHOD)arf_set_default_random_engine_type, 0);  rb_define_singleton_method(Random, \"random_engine_get_seed\", (METHOD)arf_random_engine_get_seed, 0);  rb_define_singleton_method(Random, \"release_random_engine\", (METHOD)arf_release_random_engine, 0);  rb_define_singleton_method(Random, \"randu\", (METHOD)arf_randu, 2);  rb_define_singleton_method(Random, \"randn\", (METHOD)arf_randn, 2);  rb_define_singleton_method(Random, \"set_seed\", (METHOD)arf_set_seed, 1);  rb_define_singleton_method(Random, \"get_seed\", (METHOD)arf_get_seed, 0);}A Random enigne must be created so I write the ruby bindings to alloc memoryand dealloc memory to a af_random_engine using arf_engine_alloc and arf_engine_freerespectively. The rest is similar to how I created bindings in previous blog posts.typedef struct RANDOM_ENGINE_STRUCT{  af_random_engine cengine;}afrandomenginestruct;static VALUE arf_engine_alloc(VALUE klass){  /* allocate */  afrandomenginestruct* afrandomengine = ALLOC(afrandomenginestruct);  /* wrap */  return Data_Wrap_Struct(klass, NULL, arf_engine_free, afrandomengine);}static void arf_engine_free(afrandomenginestruct* afrandomengine){  free(afrandomengine);}static VALUE arf_create_random_engine(VALUE self, VALUE type_val, VALUE seed_val){  afrandomenginestruct* output = ALLOC(afrandomenginestruct);  af_random_engine_type rtype = arf_randome_engine_type_from_rbsymbol(type_val);  af_create_random_engine(&amp;output-&gt;cengine, AF_RANDOM_ENGINE_DEFAULT, NUM2ULL(seed_val) ) ;  return Data_Wrap_Struct(Random, NULL, arf_engine_free, output);}static VALUE arf_randu(VALUE self, VALUE ndims_val, VALUE dim_val){  afstruct* out_array = ALLOC(afstruct);  dim_t ndims = (dim_t)FIX2LONG(ndims_val);  dim_t* dimensions = (dim_t*)malloc(ndims * sizeof(dim_t));  dim_t count = 1;  for (dim_t index = 0; index &lt; ndims; index++) {    dimensions[index] = (dim_t)FIX2LONG(RARRAY_AREF(dim_val, index));    count *= dimensions[index];  }  af_randu(&amp;out_array-&gt;carray, ndims, dimensions,f64);  return Data_Wrap_Struct(Af_Array, NULL, arf_free, out_array);}Now, we have the bindings ready, we can check it using pry.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; engine = ArrayFire::Random.create_random_engine(:AF_RANDOM_ENGINE_PHILOX_4X32_10, 100)=&gt; #&lt;ArrayFire::Random:0x00000002a41350&gt;[2] pry(main)&gt; ArrayFire::Random.random_engine_get_seed(engine)=&gt; 100[3] pry(main)&gt; ArrayFire::Random.random_engine_set_seed(engine, 123)=&gt; #&lt;ArrayFire::Random:0x00000002cfbd98&gt;[4] pry(main)&gt; ArrayFire::Random.random_engine_get_seed(engine)=&gt; 123[5] pry(main)&gt; ArrayFire::Random.random_engine_get_type engine=&gt; \"AF_RANDOM_ENGINE_PHILOX_4X32_10\"[6] pry(main)&gt; arr = ArrayFire::Random.randu(2, [4, 4])=&gt; #&lt;ArrayFire::Af_Array:0x00000002b185f8&gt;[7] pry(main)&gt; ArrayFire::Util.print_array(arr)No Name Array[4 4 1 1]    0.3990     0.7353     0.9455     0.7089    0.6720     0.5160     0.1587     0.9434    0.5339     0.3932     0.8831     0.1227    0.1386     0.2706     0.0621     0.9107=&gt; true[8] pry(main)&gt; arr2 = ArrayFire::Random.randn(2, [4, 4])=&gt; #&lt;ArrayFire::Af_Array:0x000000029fd4c0&gt;[9] pry(main)&gt; ArrayFire::Util.print_array(arr2)No Name Array[4 4 1 1]    0.2985    -0.8873    -1.0309     0.5312   -2.7126    -0.3550    -1.4627    -1.7783    0.4584     1.9841     0.0075    -0.5459    1.5579    -0.9308    -1.0512     0.4640=&gt; truewe have the Random support ready for ArrayFire-rbHence, Ruby bindings for Statistics and Random methods have been successfully implemented.ConclusionWe can now use ArrayFire-rb for statistical analysis on data. Since, the calculations would beon GPU, we can feed it large amount of data from real world.Random engine can help in creating large arrays with randomly generated values in seconds.In the next blog, I will explain about using the Device and Util class to manage GPU devicesand device memory and other utilities like saving an Af_Array to file.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/08/17/gsoc17-arrayfire-ruby-bindings-part-4-statistics-and-random-engine.html"
			      }
			      ,
			    
			      "arrayfire-2017-08-16-gsoc17-arrayfire-ruby-bindings-part-4-blas-lapack-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part IV : BLAS and LAPACK routines)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "This post covers how to use BLAS and LAPACK routines with ArrayFire-rb, and how these functionshave been implemented by creating Ruby bindings to ArrayFire C API.The performance benchmarks for ArrayFire against NMatrix can be represented by the following figures.The code used for benchmarking and generating the plots can be found hereand be used to reproduce similar plots.Performance metrics                  BLAS Routines                            LAPACK Routines                                (Note: The above benchmarks have been done on an AMD FX 8350 octacore processorand Nvidia GTX 750Ti GPU. CUDA backend of ArrayFire was used with double floating points.)The figure shows that ArrayFire takes the least computation time of all.ArrayFire is 3 e +6 times faster than NMatrix for JRuby and NMatrix for Ruby(not BLAS)whereas 7 e +5 times faster than NMatrix for Ruby(using BLAS).For LAPACK routines, like calculating determinant and lower-upper factorization,ArrayFire is 100 times faster than NMatrix for JRubywhereas 6 times faster than NMatrix for Ruby(using LAPACKE).Lets take a look at the implementation.BLAS RoutinesAll the BLAS methods are singleton_methods.void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Blas = rb_define_class_under(ArrayFire, \"BLAS\", rb_cObject);  rb_define_singleton_method(Blas, \"matmul\", (METHOD)arf_matmul, 4);  rb_define_singleton_method(Blas, \"dot\", (METHOD)arf_dot, 4);  rb_define_singleton_method(Blas, \"transpose\", (METHOD)arf_transpose, 1);}arf_matmul and arf_transpose call af_matmul and af_transpose respectively.static VALUE arf_matmul(VALUE self, VALUE left_val, VALUE right_val, VALUE left_prop_val, VALUE right_prop_val){  afstruct* left;  afstruct* right;  afstruct* result = ALLOC(afstruct);  Data_Get_Struct(left_val, afstruct, left);  Data_Get_Struct(right_val, afstruct, right);  af_mat_prop left_mat_prop = arf_mat_type_from_rbsymbol(left_prop_val);  af_mat_prop right_mat_prop = arf_mat_type_from_rbsymbol(right_prop_val);  af_matmul(&amp;result-&gt;carray, left-&gt;carray, right-&gt;carray, left_mat_prop, right_mat_prop);  af_print_array(result-&gt;carray);  return Data_Wrap_Struct(CLASS_OF(left_val), NULL, arf_free, result);}static VALUE arf_transpose(VALUE self, VALUE input){  afstruct* obj;  afstruct* result = ALLOC(afstruct);  Data_Get_Struct(input, afstruct, obj);  af_transpose(&amp;result-&gt;carray, obj-&gt;carray, false);  af_print_array(result-&gt;carray);  return Data_Wrap_Struct(CLASS_OF(input), NULL, arf_free, result);}af_matmul expects the property of matrices passed to it. For example, af_mat_prop AF_MAT_TRANSsignifies that the matrix is transposed. So, to pass the symbols to Ruby C function arf_matmul, Ihave used the code given below. I have creared a list of values MAT_PROPERTIES and a functionarf_mat_type_from_symbol which is responsible for figuring out af_mat_prop from the symbol.std::map&lt;char*, size_t&gt; MAT_PROPERTIES = {  {\"AF_MAT_NONE\", 0},  {\"AF_MAT_TRANS\", 1},  {\"AF_MAT_CTRANS\", 2},  {\"AF_MAT_CONJ\", 4},  {\"AF_MAT_UPPER\", 32},  {\"AF_MAT_LOWER\", 64},  {\"AF_MAT_DIAG_UNIT\", 128},  {\"AF_MAT_SYM\", 512},  {\"AF_MAT_POSDEF\", 1024},  {\"AF_MAT_ORTHOG\", 2048},  {\"AF_MAT_TRI_DIAG\", 4096},  {\"AF_MAT_BLOCK_DIAG\", 8192}};af_mat_prop arf_mat_type_from_rbsymbol(VALUE sym) {  ID sym_id = SYM2ID(sym);  for(std::map&lt;char*, size_t&gt;::value_type&amp; entry : MAT_PROPERTIES) {    if (sym_id == rb_intern(entry.first)) {      return static_cast&lt;af_mat_prop&gt;(entry.second);    }  }  VALUE str = rb_any_to_s(sym);  rb_raise(rb_eArgError, \"invalid matrix type symbol (:%s) specified\",   RSTRING_PTR(str));}So, now I can check if the bindings work successfully.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; left = ArrayFire::Af_Array.new 2 , [3,3] , [1, 4, 6, 4, 11 , 2 ,-5, 8, 10]No Name Array[3 3 1 1]    1.0000     4.0000    -5.0000    4.0000    11.0000     8.0000    6.0000     2.0000    10.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000014e56c8&gt;[2] pry(main)&gt; right = ArrayFire::Af_Array.new 2 , [3,2] , [1, 0, 8, 10, -11, 8]No Name Array[3 2 1 1]    1.0000    10.0000    0.0000   -11.0000    8.0000     8.0000=&gt; #&lt;ArrayFire::Af_Array:0x00000001591db0&gt;[3] pry(main)&gt; result = ArrayFire::BLAS.matmul(left, right, :AF_MAT_NONE, :AF_MAT_NONE)No Name Array[3 2 1 1]  -39.0000   -74.0000   68.0000   -17.0000   86.0000   118.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000016136f8&gt;[4] pry(main)&gt; transposed = ArrayFire::BLAS.transpose(left)No Name Array[3 3 1 1]    1.0000     4.0000     6.0000    4.0000    11.0000     2.0000   -5.0000     8.0000    10.0000=&gt; #&lt;ArrayFire::Af_Array:0x00000001762f68&gt;It works!.LAPACK RoutinesLAPACK routines similar to BLAS ones are singleton methods.Lapack = rb_define_class_under(ArrayFire, \"LAPACK\", rb_cObject);rb_define_singleton_method(Lapack, \"svd_func\", (METHOD)arf_svd_func, 4);rb_define_singleton_method(Lapack, \"svd_inplace_func\", (METHOD)arf_svd_inplace_func, 1);rb_define_singleton_method(Lapack, \"lu_func\", (METHOD)arf_lu_func, 4);rb_define_singleton_method(Lapack, \"lu_inplace_func\", (METHOD)arf_lu_inplace_func, 1);rb_define_singleton_method(Lapack, \"qr_func\", (METHOD)arf_qr_func, 4);rb_define_singleton_method(Lapack, \"qr_inplace_func\", (METHOD)arf_qr_inplace_func, 1);rb_define_singleton_method(Lapack, \"cholesky_func\", (METHOD)arf_cholesky_func, 3);rb_define_singleton_method(Lapack, \"cholesky_inplace_func\", (METHOD)arf_cholesky_inplace_func, 1);rb_define_singleton_method(Lapack, \"solve_func\", (METHOD)arf_solve_func, 0);rb_define_singleton_method(Lapack, \"solve_lu_func\", (METHOD)arf_solve_lu_func, 0);rb_define_singleton_method(Lapack, \"inverse\", (METHOD)arf_inverse, 1);rb_define_singleton_method(Lapack, \"rank\", (METHOD)arf_rank, 1);rb_define_singleton_method(Lapack, \"det\", (METHOD)arf_det, 1);rb_define_singleton_method(Lapack, \"norm\", (METHOD)arf_norm, 1);rb_define_singleton_method(Lapack, \"is_lapack_available\", (METHOD)arf_is_lapack_available, 0);LAPACK.svd must return multiple objects and hence calls LAPACK.arf_svd_func. The implementationis as follows.static VALUE arf_svd_func(VALUE self, VALUE u_val, VALUE s_val, VALUE vt_val, VALUE val){  afstruct* input;  afstruct* u;  afstruct* s;  afstruct* vt;  Data_Get_Struct(val, afstruct, input);  Data_Get_Struct(u_val, afstruct, u);  Data_Get_Struct(s_val, afstruct, s);  Data_Get_Struct(vt_val, afstruct, vt);  af_svd(&amp;u-&gt;carray, &amp;s-&gt;carray, &amp;vt-&gt;carray, input-&gt;carray);  return Qtrue;}module ArrayFire  class LAPACK    def self.svd(af_array)      u =  ArrayFire::Af_Array.new      s =  ArrayFire::Af_Array.new      vt = ArrayFire::Af_Array.new      ArrayFire::LAPACK.svd_func(u, s, vt, af_array)      return u, s, vt    end  endendLAPACK.det calls af_det to calculate the determinant. Currently, I haven’tcreated support for complex dtype. So, I have ignored the imaginary part ofthe result if it is not a real number.static VALUE arf_det(VALUE self, VALUE val){  afstruct* matrix;  double det_real, det_imag;  Data_Get_Struct(val, afstruct, matrix);  af_det(&amp;det_real, &amp;det_imag, matrix-&gt;carray);  return DBL2NUM(det_real);}Now, we can check the elementwise operations using pry.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; matrix = ArrayFire::Af_Array.new 2, [4,2], [1,3,5,7,2,4,6,8]No Name Array[4 2 1 1]    1.0000     2.0000    3.0000     4.0000    5.0000     6.0000    7.0000     8.0000=&gt; #&lt;ArrayFire::Af_Array:0x0000000265cbe0&gt;[2] pry(main)&gt; u , s, vt = ArrayFire::LAPACK.svd(matrix)=&gt; [#&lt;ArrayFire::Af_Array:0x000000026ff070&gt;, #&lt;ArrayFire::Af_Array:0x000000026ff048&gt;, #&lt;ArrayFire::Af_Array:0x000000026ff020&gt;][3] pry(main)&gt; ArrayFire::Util.print_array(u)No Name Array[4 4 1 1]   -0.1525    -0.8226    -0.3945    -0.3800   -0.3499    -0.4214     0.2428     0.8007   -0.5474    -0.0201     0.6979    -0.4614   -0.7448     0.3812    -0.5462     0.0407=&gt; true[4] pry(main)&gt; ArrayFire::Util.print_array(s)No Name Array[2 1 1 1]   14.2691    0.6268=&gt; true[5] pry(main)&gt; ArrayFire::Util.print_array(vt)No Name Array[2 2 1 1]   -0.6414    -0.7672    0.7672    -0.6414=&gt; true[6] pry(main)&gt; left = ArrayFire::Af_Array.new 2 , [3,3] , [1, 4, 6, 4, 11 , 2 ,-5, 8, 10]No Name Array[3 3 1 1]    1.0000     4.0000    -5.0000    4.0000    11.0000     8.0000    6.0000     2.0000    10.0000=&gt; #&lt;ArrayFire::Af_Array:0x00000002984c78&gt;[7] pry(main)&gt; det = ArrayFire::LAPACK.det(left)=&gt; 415.9999694824219It works!ConclusionWe can now use ArrayFire-rb for linear algebra since the BLAS and LAPACK routines are inplace. Also, the performance is outstanding.In the next blog, I will explain about using the Random Engine generator and Statistics methods for ArrayFire.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/08/16/gsoc17-arrayfire-ruby-bindings-part-4-blas-lapack.html"
			      }
			      ,
			    
			      "gpu-computing-2017-07-25-gsoc17-calling-d-from-ruby-for-gpu-computing-html": {
			        "title": "Calling D from Ruby for GPU computing",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "todo:I have been working on “Port NMatrix to JRuby” as my GSoC project. I am pleased to announce that JRuby is ready for Nmatrix users.NMatrix, a linear algebra library wraps Apache Commons Maths for its core functionalities. By the end of GSoC, I have been able to implement NMatrix for dense matrices with double and object ( ruby objects ) data type. I have also worked on porting mixed-models gem to JRuby which heavily uses NMatrix at its core.As ArrayFire-rb, a GPGPU libray has started taking shape,the next goal is to interface it with NMatrix library, so we canrun Linear Mixed Models(mixed_models).I have also written an LMM(Linear Mixed Models) solver in D for Genome Wide Association studiescalled faster_lmm_d which has two GPU backends:      CUDA backend which helps it directly interact with CUBLAS libraries and runs only on Nvidia Hardware.For CUDA backend, Faster_LMM_D uses cuda_d(The D bindings I wrote for CUDA libraries).        ArrayFire backend which helps it run on all major GPU vendors(Nvidia, Intel, AMD) by calling CUDA, CuBLAS,OpenCL, clBLAS libraries using the ArrayFire library. For ArrayFire backend, Faster_LMM_D usesarrayfire-d(The D bindings I wrote for ArrayFire library).  So, I found that it would be really great if we could port faster_lmm_d to Ruby by calling D from Ruby. faster_lmm_duses some cool tricks to efficiently use GPUs for computing like minimizing the warmup time, minimal copyingof CPU memory to GPU memory. It logs CPU memory and GPU memory before and after every significant computation.So, it would be an ideal exercise for GPU computing on Ruby by calling D functions under the hood.Although, this is still a work in progress, I would like to share about my experiences.Previous AttemptsThere has been previous attempts at Calling D from Ruby. In my search for the existing attempts at“Calling D from Ruby”, I stumbled upon these resources.  FFI: Use the Ruby-FFI gem to call a D function from C.  RuDy: An effort / library / gem to enableand ease writing Ruby native extensions in D programming language. It depends on creatind D bindings for the API calls in ruby.h headerfile.  Ruby-Dlang: Similar approach as Rudy.  Orbit: Similar approach as Rudy.I have been intereseted in the first two methods, i.e. FFI and Rudy.Exploring FFIThe idea here is to create a shared object .so and call it with Ruby by loading the shared object using FFI gem.Dlang has three compilers namely DMD, GDC and LDC. The post on Dlang wiki makes use of DMD, I could easilyrun the example on my machine using DMD, however I could not run it with LDC. I need to compile the d fileswith LDC because faster_lmm_d makes use of LDC compiler switches and it would take a lot of effort to be able torun it with DMD.The error I encountered is:  Error with ld :    $ ldc2 -shared -m64 -relocation-model=pic i.d/usr/bin/ld: /home/prasun/ldclatest/ldc-1.1.0-pk9rkm4zvdp6pglam7s2/lib/libdruntime-ldc.a(errno.c.o): relocation R_X86_64_PC32 against undefined symbol `__errno_location@@GLIBC_2.2.5' can not be used when making a shared object; recompile with -fPIC/usr/bin/ld: final link failed: Bad valuecollect2: error: ld returned 1 exit statusError: /usr/lib/nvidia-cuda-toolkit/bin/gcc failed with status: 1      I have also tried some hit and trial and yet I was unable to properly compile the file.Exploring RuDyI find Rudy as a more suitable way to create Ruby native extensions.When creating a Ruby native extension in C, we have a set of APIs provided by ruby.h that helps in creating aModule or a Class and the methods are nested to it.For example, rb_define_module helps in creating a module and rb_define_class_under is used to create a classnested in a module. rb_define_method and rb_define_singleton_method are used to create methods. The variablesare used as VALUE which are passed around between Ruby frontend and C backend.A simple code snippet.// calculater.cvoid Init_extension() {  Calculator = rb_define_module(\"Calculator\");              // Calculator module  IO = rb_define_class_under(Calculator, \"IO\", rb_cObject); // Calculator::IO class  rb_define_alloc_func(IO, io_alloc);                       // memory allocator for C structs  rb_define_method(IO, \"initialize\", io_init, -1);          // Calculator::IO#new constructor  rb_define_method(IO, \"get_stream\",io_get_stream,1 );      // Calculator::IO#get_stream method}static VALUE io_init(int argc, VALUE* argv, VALUE self){  // code...  return self;}static VALUE io_get_stream(VALUE self, VALUE some_val){  return Qtrue;}After the C code with the module, classes and methods are in place, we compile it to get a shared object extension.so. Then a Rubyfile calculater.rb can just load that extension.so as:# calculater.rbrequire 'calculater.so'This seems simple but as the size of application grows, the extension becomes tough to manage.I believe D to be a superior C and provides modern programming paradigm and the LDC compiler is very promising. So, it would be great if we could write thenative extension in D and get a shared library. Also, LDC has an incredible garbage collecter that would be very helpful for scientific computing Ruby libraries.To write the extension in D, the ruby.h bindings could be created in D using dstep or bcd.RuDy doesn’t have complete supportfor LDC and I am still getting my head around using RuDy to create Ruby native extensions. Here is a nice blog post about RuDy project.ConclusionCalling D from Ruby would for sure prove to be an efficient way of writing Ruby native extensions and use D as abetter choice over C. In the meanwhile, porting faster-lmm-d is a work in progress and I will share further updateswhen it is ready.The progress of Ruby port of faster-lmm-d can be tracked at Bio-faster_lmm_dand would become a part of BioRuby project. The latest progress can be tracked here.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/gpu-computing/2017/07/25/gsoc17-calling-d-from-ruby-for-gpu-computing.html"
			      }
			      ,
			    
			      "arrayfire-2017-07-22-gsoc17-arrayfire-ruby-bindings-part-3-minitest-algorithm-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part III : Test-suite and Algorithm class)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "This post covers how I implemented the test-suite and Algorithm class for ArrayFire-rb using  ArrayFire C API, andhow to use Algorithm class methods with Af_Array.Test suiteI have used minitest for writing the tests for ArrayFire-rb.To assert if two arrays are equal, I implemented Af_Array#== method. I used af_eq api for implementing ==.However af_eq method checks for exact matches and thusdoesn’t work if the values are correct upto a certain number of decimal places. So, I can’t use == to test the outputof Trigonometric methods , for-example, Af_Array#sin. I implemented Af_Array#approx_equal that checks if the values arecorrect upto three decimal places.This is how arith_test.rb looks like:class ArrayFire::ArithTest &lt; Minitest::Test  def setup    @a = ArrayFire::Af_Array.new 2, [2,2],[1,2,3,4]    @b = ArrayFire::Af_Array.new 2, [2,2],[2,4,6,8]    @elements = [10, -11, 48, 21, 65, 0, 1, -7, 112]    @af_array =  ArrayFire::Af_Array.new 2, [3,3], @elements  end  def test_addition    c = ArrayFire::Af_Array.new 2, [2,2],[3,6,9,12]    assert_equal c, @a + @b  end  def test_subtraction    assert_equal @a, @b - @a  end  def test_multiplication    c = ArrayFire::Af_Array.new 2, [2,2],[2,8,18,32]    assert_equal c, @b * @a  end  def test_division    c = ArrayFire::Af_Array.new 2, [2,2],[2,2,2,2]    assert_equal c, @b / @a  end  [:sin, :cos, :tan, :sinh, :cosh, :tanh].each do |method|    define_method(\"test_#{method}\") do      x = @elements.map{ |e| Math.send(method, e) }      res_arr =  ArrayFire::Af_Array.new 2, [3,3], x      assert res_arr.approx_equal @af_array.send method    end  endendThe implementation of == and approx_equal are as follows:rb_define_method(Af_Array, \"==\",(METHOD)arf_eqeq,1);rb_define_method(Af_Array, \"approx_equal\",(METHOD)arf_eqeq_approx,1);static VALUE arf_eqeq(VALUE left_val, VALUE right_val) {  afstruct* left;  afstruct* right;  afstruct* result = ALLOC(afstruct);  Data_Get_Struct(left_val, afstruct, left);  Data_Get_Struct(right_val, afstruct, right);  af_eq(&amp;result-&gt;carray,  left-&gt;carray, right-&gt;carray, true);  dim_t count;  af_get_elements(&amp;count, result-&gt;carray);  bool* data = (bool*)malloc(count * sizeof(bool));  af_get_data_ptr(data, result-&gt;carray);  for (dim_t index = 0; index &lt; count; index++){    if(!data[index]){      return Qfalse;    }  }  return Qtrue;}static VALUE arf_eqeq_approx(VALUE left_val, VALUE right_val) {  afstruct* left;  afstruct* right;  dim_t left_count;  dim_t right_count;  Data_Get_Struct(left_val, afstruct, left);  Data_Get_Struct(right_val, afstruct, right);  af_get_elements(&amp;left_count, left-&gt;carray);  af_get_elements(&amp;right_count, right-&gt;carray);  if(left_count != right_count){return Qfalse;}  float* left_arr = (float*)malloc(left_count * sizeof(float));  af_get_data_ptr(left_arr, left-&gt;carray);  float* right_arr = (float*)malloc(left_count * sizeof(float));  af_get_data_ptr(right_arr, right-&gt;carray);  for (dim_t index = 0; index &lt; left_count; index++){    float diff = left_arr[index] - right_arr[index];    if(diff &lt; 0){diff *= -1;}    if(diff &gt; 1e-3){      return Qfalse;    }  }  return Qtrue;}Algorithm classThe Algorithm class contains of reduction methods like sum, product, max.It contains singleton methods that takes an Af_Array as a parameter.void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Algorithm = rb_define_class_under(ArrayFire, \"Algorithm\", rb_cObject);  rb_define_singleton_method(Algorithm, \"sum\", (METHOD)arf_sum, 2);  rb_define_singleton_method(Algorithm, \"sum_nan\", (METHOD)arf_sum_nan, 3);  rb_define_singleton_method(Algorithm, \"product\", (METHOD)arf_product, 2);  rb_define_singleton_method(Algorithm, \"product_nan\", (METHOD)arf_product_nan, 3);  rb_define_singleton_method(Algorithm, \"min\", (METHOD)arf_min, 2);  rb_define_singleton_method(Algorithm, \"max\", (METHOD)arf_max, 2);  rb_define_singleton_method(Algorithm, \"all_true\", (METHOD)arf_all_true, 2);  rb_define_singleton_method(Algorithm, \"any_true\", (METHOD)arf_any_true, 2);  rb_define_singleton_method(Algorithm, \"count\", (METHOD)arf_count, 2);  rb_define_singleton_method(Algorithm, \"sum_all\", (METHOD)arf_sum_all, 1);  rb_define_singleton_method(Algorithm, \"sum_nan_all\", (METHOD)arf_sum_nan_all, 2);  rb_define_singleton_method(Algorithm, \"product_all\", (METHOD)arf_product_all, 1);  rb_define_singleton_method(Algorithm, \"product_nan_all\", (METHOD)arf_product_nan_all, 2);  rb_define_singleton_method(Algorithm, \"min_all\", (METHOD)arf_min_all, 1);  rb_define_singleton_method(Algorithm, \"max_all\", (METHOD)arf_max_all, 1);}Algorithm#sum expects an Af_Array and dimension as a prameter. It finds the sum ofall elements in the Af_Array along the specified dimension. However, sum_all finds the overallsum of elements in an Af_Array.If NaN is one of the element, we use Af_Array#sum_nan or Af_Array#sum_nan_all that just takesan additional value as an input which replaces NaN.So, now I can check if the bindings work successfully.The implementation of Algorithm#sum and Algorith#sum_all is as follows:static VALUE arf_sum(VALUE self, VALUE array_val, VALUE dim_val){  afstruct* input;  afstruct* output = ALLOC(afstruct);  Data_Get_Struct(array_val, afstruct, input);  af_sum(&amp;output-&gt;carray, input-&gt;carray, FIX2INT(dim_val));  return Data_Wrap_Struct(CLASS_OF(array_val), NULL, arf_free, output);}static VALUE arf_sum_all(VALUE self, VALUE array_val){  afstruct* input;  double real_part, imag_part;  Data_Get_Struct(array_val, afstruct, input);  af_sum_all(&amp;real_part, &amp;imag_part, input-&gt;carray);  return DBL2NUM(real_part);}Data_Get_Struct takes an Af_Array value and unwraps it to a af_array struct.Next, the ArrayFire C API af_sum or af_sum_all is called on the parameters. Theresult is returned by converting it into a Ruby VALUE.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; input = ArrayFire::Af_Array.new 2, [3,3], [4, 1, 5, 6, -11, 9 , -22, 11, 1]No Name Array[3 3 1 1]    4.0000     6.0000   -22.0000    1.0000   -11.0000    11.0000    5.0000     9.0000     1.0000=&gt; #&lt;ArrayFire::Af_Array:0x0000000176b938&gt;[2] pry(main)&gt; result = ArrayFire::Algorithm.sum(input, 1)=&gt; #&lt;ArrayFire::Af_Array:0x00000001810f50&gt;[3] pry(main)&gt; result.elements=&gt; [-12.0, 1.0, 15.0][4] pry(main)&gt; result = ArrayFire::Algorithm.sum_all(input)=&gt; 4.0Voila! It works.ConclusionArrayFire-rb has a test-suite and the Algorithm class. Some of the methods of Algorithm classare missing and will be added soon.In the next blog post, I will explain about the BLAS and LAPACK routines.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/07/22/gsoc17-arrayfire-ruby-bindings-part-3-minitest-algorithm.html"
			      }
			      ,
			    
			      "arrayfire-2017-07-04-gsoc17-arrayfire-ruby-bindings-part-2-af-array-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part II : Af_Array)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "This post covers how to create arrays using arrayfire-rb, perform elementwiseoperations like addition, multiplication, etc. and how these functions have beenimplemented by creating Ruby bindings to ArrayFire C API.ArrayFire for Ruby stores arrays upto 4 dimension in the Af_Array class.The speedup achieved is outstanding.Performance metrics                  Arithmetic Operations                            (Note: The above benchmarks have been done on an AMD FX 8350 octacore processorand Nvidia GTX 750Ti GPU.)The figure shows that ArrayFire takes the least computation time of all.For elementwise arithmetic operations,  ArrayFire is 2 e 4 times faster than NMatrix for Ruby whereas2 e 3 times faster than NMatrix for JRuby.The performance benchmarks for ArrayFire against NMatrix can be represented by the following figures.The code used for benchmarking and generating the plots can be found hereand be used to reproduce similar plots.Lets take a look at the implementation.InitializationAn Af_Array expects the number of dimensions( ndims ), size of array along eachdimension(dimension) and the elements(elements).Af_Array class is created under the ArrayFire module using rb_define_class_under().Next, I have added arf_alloc function using rb_define_alloc_func that allocatesmemory  to Af_Array and is run everytime Af_Array#new is called. arf_alloc worksalong with arf_init that accepts parameters from Ruby call and calls C to createan af_array. ArrayFire C apis use af_array pointer to store an array.The methods for elemetwise operations have also been implemented using rb_define_method.void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  Af_Array = rb_define_class_under(ArrayFire, \"Af_Array\", rb_cObject);  rb_define_alloc_func(Af_Array, arf_alloc);  rb_define_method(Af_Array, \"initialize\", (METHOD)arf_init, -1);  rb_define_method(Af_Array, \"+\",(METHOD)arf_ew_add,1);  rb_define_method(Af_Array, \"-\",(METHOD)arf_ew_subtract,1);  rb_define_method(Af_Array, \"*\",(METHOD)arf_ew_multiply,1);  rb_define_method(Af_Array, \"/\",(METHOD)arf_ew_divide,1);}To store an Af_Array, I have create an afstruct that stores the af_array pointer.arf_init(int argc, VALUE* argv, VALUE self) can take any number of arguments and selfis used to bind the afstruct to an Af_Array object.The Ruby C APIs uses VALUE to pass around pointers. I have casted all the VALUE typesto the C types expected by ArrayFire C API. NUM2LONG and NUM2DBL have been used toconvert the VALUE to long and double respectively.Once, I have the ndims, dimensions and elements, I can use af_create_array to create anarray. afstruct-&gt;carray points to the array and the array can be accessed anytime byusing this pointer.The point to note here is that all the data is on GPU now and hence, the time and resourcesfor copying data from GPU to CPU is taken care of. It may not be clear now but it will bepivotal in the blogs to come when I interface mixed_models with arrayfire.typedef struct AF_STRUCT{  af_array carray;}afstruct;VALUE arf_init(int argc, VALUE* argv, VALUE self){  afstruct* afarray;  Data_Get_Struct(self, afstruct, afarray);  dim_t ndims = (dim_t)NUM2LONG(argv[0]);  dim_t* dimensions = (dim_t*)malloc(ndims * sizeof(dim_t));  dim_t count = 1;  for (size_t index = 0; index &lt; ndims; index++) {    dimensions[index] = (dim_t)NUM2LONG(RARRAY_AREF(argv[1], index));    count *= dimensions[index];  }  float* host_array = (float*)malloc(count * sizeof(float));  for (size_t index = 0; index &lt; count; index++) {    host_array[index] = (float)NUM2DBL(RARRAY_AREF(argv[2], index));  }  af_create_array(&amp;afarray-&gt;carray, host_array, ndims, dimensions, f32);  af_print_array(afarray-&gt;carray);  return self;}static VALUE arf_alloc(VALUE klass){  /* allocate */  afstruct* af = ALLOC(afstruct);  /* wrap */  return Data_Wrap_Struct(klass, NULL, arf_free, af);}So, now I can check if the bindings work successfully.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; a = ArrayFire::Af_Array.new 2, [2,2],[1,2,3,4]No Name Array[2 2 1 1]   Offsets: [0 0 0 0]   Strides: [1 2 4 4]    1.0000     3.0000    2.0000     4.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000020aeab8&gt;[2] pry(main)&gt;(Note: ArrayFire stores array in column-major format.)Voila! It works.Elementwise operationsThe following code snippets show how I implemented elemetwise operations likeaddition, subtraction, multiplication and division.I have created macros DEF_ELEMENTWISE_RUBY_ACCESSOR and  DECL_ELEMENTWISE_RUBY_ACCESSORthatdefine and declare the functions. The function names call the correspondingArrayFire API.e.g. Af_Array#+ calls arf_ew_add which is responsible for calling af_add.#define DEF_ELEMENTWISE_RUBY_ACCESSOR(name, oper)                          \\static VALUE arf_ew_##name(VALUE left_val, VALUE right_val) {              \\  afstruct* left;                                                          \\  afstruct* right;                                                         \\  afstruct* result = ALLOC(afstruct);                                      \\  Data_Get_Struct(left_val, afstruct, left);                               \\  Data_Get_Struct(right_val, afstruct, right);                             \\  af_##oper(&amp;result-&gt;carray,  left-&gt;carray, right-&gt;carray, true);          \\  af_print_array(result-&gt;carray);                                          \\  return Data_Wrap_Struct(CLASS_OF(left_val), NULL, arf_free, result);     \\}#define DECL_ELEMENTWISE_RUBY_ACCESSOR(name)                               \\static VALUE arf_ew_##name(VALUE left_val, VALUE right_val);DECL_ELEMENTWISE_RUBY_ACCESSOR(add)DECL_ELEMENTWISE_RUBY_ACCESSOR(subtract)DECL_ELEMENTWISE_RUBY_ACCESSOR(multiply)DECL_ELEMENTWISE_RUBY_ACCESSOR(divide)DEF_ELEMENTWISE_RUBY_ACCESSOR(add, add)DEF_ELEMENTWISE_RUBY_ACCESSOR(subtract, sub)DEF_ELEMENTWISE_RUBY_ACCESSOR(multiply, mul)DEF_ELEMENTWISE_RUBY_ACCESSOR(divide, div)Now, we can check the elementwise operations using pry.$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; a = ArrayFire::Af_Array.new 2, [2,2],[1,2,3,4]No Name Array[2 2 1 1]   Offsets: [0 0 0 0]   Strides: [1 2 4 4]    1.0000     3.0000    2.0000     4.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000020a3e38&gt;[2] pry(main)&gt; b = a + aNo Name Array[2 2 1 1]   Offsets: [0 0 0 0]   Strides: [1 2 4 4]    2.0000     6.0000    4.0000     8.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000020625c8&gt;[3] pry(main)&gt; b = a * aNo Name Array[2 2 1 1]   Offsets: [0 0 0 0]   Strides: [1 2 4 4]    1.0000     9.0000    4.0000    16.0000=&gt; #&lt;ArrayFire::Af_Array:0x00000001fe6f90&gt;It works!Lets check it for 4 dimensional matrices$ rake prypry -r './lib/arrayfire.rb'[1] pry(main)&gt; a = ArrayFire::Af_Array.new 4, [2,2,2,2], [1,2,3,4,                                                          5,6,7,8,                                                          9,10,11,12,                                                          13,14,15,16]No Name Array[2 2 2 2]   Offsets: [0 0 0 0]   Strides: [1 2 4 8]    1.0000     3.0000    2.0000     4.0000    5.0000     7.0000    6.0000     8.0000    9.0000    11.0000   10.0000    12.0000   13.0000    15.0000   14.0000    16.0000=&gt; #&lt;ArrayFire::Af_Array:0x000000016c7a40&gt;[2] pry(main)&gt; b = a + aNo Name Array[2 2 2 2]   Offsets: [0 0 0 0]   Strides: [1 2 4 8]    2.0000     6.0000    4.0000     8.0000   10.0000    14.0000   12.0000    16.0000   18.0000    22.0000   20.0000    24.0000   26.0000    30.0000   28.0000    32.0000=&gt; #&lt;ArrayFire::Af_Array:0x00000001686f18&gt;Hence, Af_Array can successfully handle arrays upto 4 dimesnions.ConclusionArrayFire for Ruby can successfully create arrays on GPU using Af_Array class and supportselementwise binary operations. Similarly, I have implemented elementwise unary operations like Af_Array#sinAf_Array#erfc.In the next blog post, I will explain about the test-suite and Algorithm class.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/07/04/gsoc17-arrayfire-ruby-bindings-part-2-af_array.html"
			      }
			      ,
			    
			      "arrayfire-2017-06-23-gsoc17-arrayfire-ruby-bindings-part-1-installation-html": {
			        "title": "ArrayFire Ruby Bindings&lt;br&gt;(Part I : Installation)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "I have been working on creating ArrayFire bindings for Ruby. ArrayFire is an opensourcelibrary that is very useful and highly popular for GPGPU computings. It has strong abstractions that makesit very easy for a programmer to benefit from GPU without being bothered about the configurations.I will be creating a blog series regarding that will explain how I created Ruby and JRuby bindings forArrayFire and how Ruby programmers can use ArrayFire to get free and easy speed improvements with minimumeffort. I will start with MRI bindings.Let the quest begin!To use ArrayFire, the first thing you need to do is install it.Installation :On a Debian machine, I would dosudo apt-get install libarrayfire-opencl3sudo apt-get install libarrayfire-opencl-devFor other OS, you need to checkout this linkBuilding from sourcegit clone https://github.com/prasunanand/arrayfire-rbcd arrayfire-rbbundle installrake compileInstalling the gemgem build arrayfire.gemspecgem install arrayfire-0.0.0.gemNow you have got arrayFire installed on the machine, lets check it by running thispiece of code in pry. We want to make sure that ArrayFire can detect the GPU deviceson our machine.pry[1] pry(main)&gt; require 'arrayfire'=&gt; true[2] pry(main)&gt; ArrayFire::Device.info()ArrayFire v3.4.0 (OpenCL, 64-bit Linux, build 75cad40)[0] NVIDIA  : GeForce GTX 750 Ti, 4041 MB=&gt; nilruby device.rbThis piece of Ruby code will output the GPU devices that are available onyour machine.So, now we get into the implementation:An introduction to how we setup the project can be found here.We create the bindings for ArrayFire::Device class;VALUE Device = Qnil;static VALUE arf_info(VALUE self);# Creating the Device classDevice = rb_define_class_under(ArrayFire, \"Device\", rb_cObject);# Creating singleton method inforb_define_singleton_method(Device, \"info\", (METHOD)arf_info, 0);static VALUE arf_info(VALUE self){  #C API to get device info  af_info();  return Qnil;}Introducing Af_ArrayNow, lets have a look at Af_Array. An Af_Array currently expects a two-dimensionalarray. I have implemented matrix addition and matrix multiplication.You can try it as follows:pry[1] pry(main)&gt; require 'arrayfire'=&gt; true[2] pry(main)&gt; a = ArrayFire::Af_Array.new 2, [2,2],[1,2,3,4]=&gt; #&lt;ArrayFire::Af_Array:0x00000001aa3b50&gt;[3] pry(main)&gt; b = ArrayFire::Af_Array.new 2, [2,2],[1,2,3,4]=&gt; #&lt;ArrayFire::Af_Array:0x00000001970738&gt;[4] pry(main)&gt; c = a + b=&gt; #&lt;ArrayFire::Af_Array:0x0000000191a1a8&gt;[5] pry(main)&gt; c.elements=&gt; [2.0, 4.0, 6.0, 8.0][6] pry(main)&gt; d = ArrayFire::BLAS.matmul(a,b)=&gt; #&lt;ArrayFire::Af_Array:0x00000001626258&gt;[7] pry(main)&gt; d.elements=&gt; [7.0, 10.0, 15.0, 22.0]Please note that there may be issues with Intel GPUs that don’t support double precision decimals.ConclusionWe were successfully able to run ArrayFire using Ruby and could detect the GPUhardware available. We were also able to create an Af_Array object. We couldadd two arrays and also do matrix multiplication.In the next blog, I would create bindings for Array class and how I implemetedBLAS routines and Arithmetic operations.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/arrayfire/2017/06/23/gsoc17-arrayfire-ruby-bindings-part-1-installation.html"
			      }
			      ,
			    
			      "ruby-c-extensions-2017-06-23-gsoc17-ruby-c-extensions-for-complex-projects-html": {
			        "title": "Ruby C extensions for complex projects",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "In this blog and a few others that will follow, I would share my expereiences creatingRuby Bindings for C/C++ libraries. It covers a practical example rather than aHello world! or chairs and tables examples.When I started working on ArrayFire bindings for Ruby, I was searching onthe Internet How to create Ruby C extensions? I was able to learn the basics butI wasn’t able to figure out how to implement it for a practical application.For example, if you are working on a  Number Cruncing library, you need to handledata of different types like double, float, int, unsigned int, complex. To handleall these data types, to prevent repeatition of code, we need templates. But  C doesn’tsupport templates and you need to use C++ to do it.  But you never learnt to bind C++ toRuby.Hence, I am writing this blog post to explain how to engineer complex Ruby applications that takeadvantage of C/C++ libraries. I will explain it by citing examples from NMatrix,and ArrayFire.BasicsThe first step of creating a Ruby binding is to create a shared object file that binds toyour ruby code. The shared object contains information about your Ruby Modules, Classes,Methods and Variables.An example tutorial which I like a lot, can be seen here and here.Now the challenging task would be how you will use C++, if you want to use Object Orientedparadigm of C++.Directory Structure├── ext│   └── mri│       ├── arith.cpp│       ├── arrayfire.c│       ├── blas.cpp│       ├── extconf.rb│       ├── lapack.cpp│       ├── mkmf.rb│       ├── ruby_arrayfire.cpp│       └── ruby_arrayfire.h├── lib│   ├── arrayfire│   │   └── arrayfire.rb│   └── arrayfire.rb└── RakefileCompilingNMatrix uses mkmf.rb filewhich helps you compile C++ and C code. If you want to explore more, you can look into what this codedoes.mkmf.rb helps you create an entry to shared object is through a C++ files that also contains theC files. The C file contains all the directives responsible for creating Ruby modules, classes, methods and variables and to prevent mangling we use extern to refer C and C++ code.MakefileThe extconf.rb needs to have a little modifcations that would help the Makefile find thesource files, dependencies and output the .so file.extconf.rbrequire_relative 'mkmf.rb'extension_name = 'arrayfire'$INSTALLFILES = [  ['ruby_arrayfire.h'       , '$(archdir)'],  ['ruby_arrayfire.hpp'     , '$(archdir)'],  ['arrayfire_config.h', '$(archdir)'],]$DEBUG = true$CFLAGS = [\"-Wall -Werror=return-type\",$CFLAGS].join(\" \")$CXXFLAGS = [\"-Wall -Werror=return-type\",$CXXFLAGS].join(\" \")$CPPFLAGS = [\"-Wall -Werror=return-type\",$CPPFLAGS].join(\" \")LIBDIR      = RbConfig::CONFIG['libdir']INCLUDEDIR  = RbConfig::CONFIG['includedir']HEADER_DIRS = [  '/opt/local/include',  '/usr/local/include',  INCLUDEDIR,  '/usr/include']LIB_DIRS = [  '/opt/local/lib',  '/usr/local/lib',  LIBDIR,  '/usr/lib']dir_config(extension_name, HEADER_DIRS, LIB_DIRS)have_library('afcuda')have_library('cusolver')have_library('cudart')have_library('cufft')have_library('cublas')basenames = %w{ruby_arrayfire}$objs = basenames.map { |b| \"#{b}.o\"   }$srcs = basenames.map { |b| \"#{b}.cpp\" }create_conf_h(\"arrayfire_config.h\")create_makefile(extension_name)Return typeIn the headerfile, we can add info regarding how to cast the objects and juggle themeasily between C and C++ code.Each Ruby binding must return a VALUE and we need to cast it which can be seen in thefollowing lines of code.ruby_arrayfire.h#ifndef RUBY_ARRAYFIRE_H  #define RUBY_ARRAYFIRE_H#endif/* * Functions*/#ifdef __cplusplustypedef VALUE (*METHOD)(...);#endif#include &lt;ruby.h&gt;#ifdef __cplusplusextern \"C\" {#endif  void Init_arrayfire();#ifdef __cplusplus}#endifName ManglingNow since you are loading the C file through a C++ interface you need to take care of casting thereturn value from an expression.A C++ compiler distinguishes between different functions when it generates object code byadding extra information about arguments to the function name, which is called Name Mangling.Hence, whenever we import C code, we place it in extern \"C\" block.The following code shows how to use it.Here we create an arf namespace and we create a method test_cpp that will print“Set up is successful!”. arf::test_cpp will be called by ArrayFire#test. test methodwill be defined in C and will call arf::test_cpp.ruby_arrayfire.cpp#include &lt;ruby.h&gt;#include &lt;algorithm&gt;#include &lt;fstream&gt;#include &lt;arrayfire.h&gt;#include &lt;stdio.h&gt;#include &lt;math.h&gt;/* * Project Includes */#include \"arrayfire.h\"#include \"ruby_arrayfire.h\"namespace arf {  static void test_cpp()  {    printf(\"Setup is successful!\");  }}extern \"C\" {  #include \"arrayfire.c\"}Ruby C bindingsThis is the piece of code where we create the Ruby bindings.arrayfire.cVALUE ArrayFire = Qnil;static VALUE test(VALUE self);void Init_arrayfire() {  ArrayFire = rb_define_module(\"ArrayFire\");  rb_define_method(ArrayFire, \"test\", (METHOD)test, 0);}VALUE test(VALUE self) {  arf::test_cpp();  return Qnil;}Loading the Shared Librarylib/arrayfire/arrayfire.rbrequire 'ext/arrayfire.so'ConclusionThis is how we have created our basic layout for Ruby C extension with C and C++.Now, we will create some arrayfire bindings.In the following blogs, I will write about building upon this codebase. I willintroduce templates and later Garbage collection .Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/ruby-c-extensions/2017/06/23/gsoc17-ruby-c-extensions-for-complex-projects.html"
			      }
			      ,
			    
			      "jruby-2016-08-22-gsoc16-project-report-port-nmatrix-to-jruby-html": {
			        "title": "GSoC 2016: Port NMatrix to JRuby",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "IntroductionI have been working on “Port NMatrix to JRuby” as my GSoC project. I am pleased to announce that JRuby is ready for Nmatrix users.NMatrix, a linear algebra library wraps Apache Commons Maths for its core functionalities. By the end of GSoC, I have been able to implement NMatrix for dense matrices with double and object ( ruby objects ) data type. I have also worked on porting mixed-models gem to JRuby which heavily uses NMatrix at its core.This blog post summarizes my work on the project with Sciruby, and reports the final status of the project.ProposalThe proposal application can be found here.Code Commitshttps://github.com/prasunanand/nmatrix/commits/jruby_portStoring n-dimensional matrices as flat arraysThe major components of a NMatrix is its shape, elements, dtype and stype. Any nmatrix when initialised, the elements are stored in flat arrays. ArrayRealVector class is used to store the elements.@s stores the elements, @shape stores the shape of array, while @dtype and @stype store the data type and storage type respectively. currently, we have nmatrix-jruby implemented for only double matrices.NMatrix-MRI uses @s which is an object containing elements, stride, offset as in C, we need to deal with the memory allocation for the arrays.Slicing and RankImplementing slicing was the toughest part of NMatrix-JRuby implementation.NMatrix@s stores the elements of a matrix as a flat_array. The elements along any dimension are accessed with the help of the stride. NMatrix#get_stride calculates the stride with the help of the dimension and shape and returns an array.def get_stride(nmatrix)  stride = Array.new()  (0...nmatrix.dim).each do |i|    stride[i] = 1;    (i+1...dim).each do |j|      stride[i] *= nmatrix.shape[j]    end  end  strideendNMatrix#[] and NMatrix#[]= are thus able to read and write the elements of a matrix. NMatrix#MRI uses the @s object which stores the stride when the nmatrix is initialised.NMatrix#[] calls the #xslice operator which calls  #get_slice operator that use the stride to find out whether we are accessing a single element or multiple elements. If  there are multiple elements #dense_storage_get then returns an NMatrix object with the elements along the dimension.NMatrix-MRI differs from NMatrix-JRuby implementation as it makes sure that memory is properly utilized as the memory needs to be properly garbage collected.def xslice(args)  result = nil  if self.dim &lt; args.length    raise(ArgumentError,\"wrong number of arguments\\       (#{args} for #{effective_dim(self)})\")  else    result = Array.new()    slice = get_slice(@dim, args, @shape)    stride = get_stride(self)    if slice[:single]      if (@dtype == :object)        result = @s[dense_storage_get(slice,stride)]      else        s = @s.toArray().to_a        result = @s.getEntry(dense_storage_get(slice,stride))      end    else      result = dense_storage_get(slice,stride)    end  end  return resultendNMatrix#[]= calls the #dense_storage_set operator which calls #get_slice operator that use the stride to find out whether we are accessing a single element or multiple elements. If there are multiple elements #set_slice recursively sets the elements of the matrix then returns an NMatrix object with the elements along the dimension.def dense_storage_set(slice, right)    stride = get_stride(self)    v_size = 1    if right.is_a?(NMatrix)      right = right.s.toArray.to_a    end    if(right.is_a?(Array))      v_size = right.length      v = right      if (dtype == :object)        # nm_register_values(reinterpret_cast&lt;VALUE*&gt;(v), v_size)      end      (0...v_size).each do |m|        v[m] = right[m]      end    else      v = [right]      if (@dtype == :object)        # nm_register_values(reinterpret_cast&lt;VALUE*&gt;(v), v_size)      end    end    if(slice[:single])      # reinterpret_cast&lt;D*&gt;(s-&gt;elements)[nm_dense_storage_pos(s, slice-&gt;coords)] = v;      pos = dense_storage_pos(slice[:coords],stride)      if @dtype == :object        @s[pos] = v[0]      else        @s.setEntry(pos, v[0])      end    else      v_offset = 0      dest = {}      dest[:stride] = get_stride(self)      dest[:shape] = shape      # dest[:elements] = @s.toArray().to_a      dense_pos = dense_storage_pos(slice[:coords],stride)      slice_set(dest, slice[:lengths], dense_pos, 0, v, v_size, v_offset)    end  endEnumeratorsNMatrix-MRI uses the C code for enumerating the elements of a matrix. However, the NMatrix-JRuby uses pure Ruby code. Currently, all the enumerators for dense matrices with real data-type have been implemented and they are properly functional.We haven’t implemented enumerators for objects currently.def each_with_indices  nmatrix = create_dummy_nmatrix  stride = get_stride(self)  offset = 0  #Create indices and initialize them to zero  coords = Array.new(dim){ 0 }  shape_copy =  Array.new(dim)  (0...size).each do |k|    dense_storage_coords(nmatrix, k, coords, stride, offset)    slice_index = dense_storage_pos(coords,stride)    ary = Array.new    if (@dtype == :object)      ary &lt;&lt; self.s[slice_index]    else      ary &lt;&lt; self.s.toArray.to_a[slice_index]    end    (0...dim).each do |p|      ary &lt;&lt; coords[p]    end    # yield the array which now consists of the value and the indices    yield(ary)  end if block_given?  nmatrix.s = @s  return nmatrix endTwo Dimensional MatricesLinear algebra is mostly about two-dimensional matrices. In NMatrix, when performing calculations in a two-dimensional matrix, a flat array is converted to a two-dimensional matrix. A two-dimensional matrix is stored as a BlockRealMatix or Array2DRowRealMatrix. Each of them has its own advantages.Getting a two-d-matrixpublic class MatrixGenerator{  public static double[][] getMatrixDouble(double[] array, int row, int col)  {    double[][] matrix = new double[row][col];    for (int index=0, i=0; i &lt; row ; i++){        for (int j=0; j &lt; col; j++){            matrix[i][j]= array[index];            index++;        }    }    return matrix;  }}Flat a two-d matrixpublic class ArrayGenerator{  public static double[] getArrayDouble(double[][] matrix, int row, int col)  {    double[] array = new double[row * col];    for (int index=0, i=0; i &lt; row ; i++){        for (int j=0; j &lt; col; j++){            array[index] = matrix[i][j];            index++;        }    }    return array;  }}Why use java method instead of Ruby method?      Memory Usage and Garbage Collection =&gt; A scientific library is memory intensive and hence, every step counts. JRuby interpreter doesn’t need to dynamically guess the data type and uses less memory, i.e around 10 times. If the memory is properly utilized; when the GC kicks in, it has to clear less memory and improves the speed.        Speed =&gt; Using java method greatly improves the speed around 1000 times, when compared to using ruby method.  OperatorsAll the operators from NMatrix-MRI have been implemented except moduli. The binary operators were easily implemented through Commons Math Api.def +(other)  result = create_dummy_nmatrix  if (other.is_a?(NMatrix))    #check dimension    raise(ShapeError, \"Cannot add matrices with different dimension\")\\    if (@dim != other.dim)    #check shape    (0...dim).each do |i|      raise(ShapeError, \"Cannot add matrices with different shapes\") \\      if (@shape[i] != other.shape[i])    end    result.s = @s.copy.add(other.s)  else    result.s = @s.copy.mapAddToSelf(other)  end  resultendTrigonometric, exponentiation and log operators with a singular argument i.e. matrix elements were implemented using mapToSelf method that that takes univariate function as an argument. mapToSelf maps every element of ArrayRealVector to the Univate operator passed to it and returns self object.def sin  result = create_dummy_nmatrix  result.s = @s.copy.mapToSelf(Sin.new())  resultendNMatrix#method(arg) was implemented using Bivariate functions provided by Commons-Maths and Java Maths library.def gamma  result = create_dummy_nmatrix  result.s = ArrayRealVector.new MathHelper.gamma(@s.toArray)  resultendimport org.apache.commons.math3.special.Gamma;public class MathHelper{  ...  public static double[] gamma(double[] arr){    double[] result = new double[arr.length];    for(int i = 0; i&lt; arr.length; i++){      result[i] = Gamma.gamma(arr[i]);    }    return result;  }  ...}DecompositionNMatrix-MRI relies on LAPACKE and ATLAS for matrix decomposition and solve functionalities. Apache Commons Math provides a different set of API for decomposing a matrix and solving an equation. for-example potrf and other LAPACKE specific functions have not been implemented as they are not required at all.Calculating determinant in NMatrix is tricky where a matrix is reduced either a Lower or Upper matrix and the diagonal elements of the matrix are multiplied to get the result. Also, the correct sign of the result whether positive or negative is taken into account, while calculating the determinanat. However, NMatrix-JRuby uses commons-math api to calculate the determinant.def det_exact  if (@dim != 2 || @shape[0] != @shape[1])    raise(ShapeError, \"matrices must be square to have a determinant defined\")    return nil  end  to_return = LUDecomposition.new(self.twoDMat).getDeterminant()endGiven below is the code, that shows how Cholesky decomposition has been implemented by using Commons Math API. Similarly, LU Decomposition and QR factorization have been implemented.Cholesky Decomposition  def factorize_cholesky    cholesky = CholeskyDecomposition.new(self.twoDMat)    l = create_dummy_nmatrix    twoDMat = cholesky.getL    l.s = ArrayRealVector.new(ArrayGenerator.getArrayDouble\\        (twoDMat.getData, @shape[0], @shape[1]))    u = create_dummy_nmatrix    twoDMat = cholesky.getLT    u.s = ArrayRealVector.new(ArrayGenerator.getArrayDouble\\      (twoDMat.getData, @shape[0], @shape[1]))    return [u,l]  endCholesky Decomposition for an NMatrix-JRuby requires the matrix to be square matrix.LUDecomposition  def factorize_lu with_permutation_matrix=nil    raise(NotImplementedError, \"only implemented for dense storage\")\\       unless self.stype == :dense    raise(NotImplementedError, \"matrix is not 2-dimensional\")\\       unless self.dimensions == 2    t = self.clone    pivot = create_dummy_nmatrix    twoDMat = LUDecomposition.new(self.twoDMat).getP    pivot.s = ArrayRealVector.new(ArrayGenerator.getArrayDouble\\    (twoDMat.getData, @shape[0], @shape[1]))    return [t,pivot]  endQRFactorization  def factorize_qr    raise(NotImplementedError, \"only implemented for dense storage\")\\       unless self.stype == :dense    raise(ShapeError, \"Input must be a 2-dimensional matrix to have\\       a QR decomposition\") unless self.dim == 2    qrdecomp = QRDecomposition.new(self.twoDMat)    qmat = create_dummy_nmatrix    qtwoDMat = qrdecomp.getQ    qmat.s = ArrayRealVector.new(ArrayGenerator.\\      getArrayDouble(qtwoDMat.getData, @shape[0], @shape[1]))    rmat = create_dummy_nmatrix    rtwoDMat = qrdecomp.getR    rmat.s = ArrayRealVector.new(ArrayGenerator.\\      getArrayDouble(rtwoDMat.getData, @shape[0], @shape[1]))    return [qmat,rmat]  endNMatrix#solveThe solve method currently uses LUDecomposition and Cholesky Decomposition for solving the equations.  def solve(b, opts = {})    raise(ShapeError, \"Must be called on square matrix\")\\       unless self.dim == 2 &amp;&amp; self.shape[0] == self.shape[1]    raise(ShapeError, \"number of rows of b must equal number\\       of cols of self\") if self.shape[1] != b.shape[0]    raise(ArgumentError, \"only works with dense matrices\") if self.stype != :dense    raise(ArgumentError, \"only works for non-integer, non-object dtypes\")\\       if integer_dtype? or object_dtype? or b.integer_dtype? or b.object_dtype?    opts = { form: :general }.merge(opts)    x    = b.clone    n    = self.shape[0]    nrhs = b.shape[1]    nmatrix = create_dummy_nmatrix    case opts[:form]    when :general, :upper_tri, :upper_triangular, :lower_tri, :lower_triangular      #LU solver      solver = LUDecomposition.new(self.twoDMat).getSolver      nmatrix.s = solver.solve(b.s)      return nmatrix    when :pos_def, :positive_definite      solver = Choleskyecomposition.new(self.twoDMat).getSolver      nmatrix.s = solver.solve(b.s)      return nmatrix    else      raise(ArgumentError, \"#{opts[:form]} is not a valid form option\")    end  endNMatrix#matrix_solveGiven we need to solved a system of linear equations                    AX = B where A is an m×n matrix, B and X are n×p matrices, we needed to solve this equation by iterating through B.NMatrix-MRI implements this functionality using NMatrix::BLAS::cblas_trsm operator. However, for NMatrix-JRuby, we implemented NMatrix#matrix_solve.  def matrix_solve rhs    if rhs.shape[1] &gt; 1      nmatrix = NMatrix.new :copy      nmatrix.shape = rhs.shape      res = []      #Solve a matrix and store the vectors in a matrix      (0...rhs.shape[1]).each do |i|        res &lt;&lt; self.solve(rhs.col(i)).s.toArray.to_a      end      #res is in col major format      result = ArrayGenerator.getArrayColMajorDouble \\         res.to_java :double, rhs.shape[0], rhs.shape[1]      nmatrix.s = ArrayRealVector.new result      return nmatrix    else      return self.solve rhs    end  endCurrently, Hessenberg transformation for an NMatix has not been implemented.Other dtypesWe have tried implementing float dtypes using jblas FloatMatrix. We here used jblas instead of commons math as Commons Math uses Field Elements for Floats and we may have faced issues with Reflection and TypeErasure. However, we had issues with precision.Code Organisation and DeploymentTo minimise conflict with the MRI codebase all the ruby code has been placed in /lib/nmatrix/jruby directory. /lib/nmatrix/nmatrix.rb decides whether to load nmatrix.so or nmatrix_jruby.rb after detecting the Ruby Platform.The added advantage of this is at run-time the ruby interpreter must not decide which function to call. The impact on performance can be seen when running programs which intensively use NMatrix for linear algebraic computations(e.g. mixed-models).PerformanceWe have benchmarked some of the NMatrix functionalities. The following plots compare the performance between NMatrix-JRuby, NMatrix-MRI and NMatrix-MRI using LAPACKE/ATLAS libraries.The lower the slope of the curve, the better is the performance.Note:  Addition and subtraction are not supported by LAPACKE/ATLAS.  NMatrix - MRI relies on LAPACKE/ATLAS for calculating determinants and LU Decomposition(lud).Result:      For, two dimensional matrices, NMatrix-JRuby is currently slower than NMatrix-MRI for matrix multiplication, and matrix decomposition functionalities(calculating determinant and factorizing a matrix). NMatrix-JRuby is faster than NMatrix-MRI for other functionalities of a two dimensional matrix, like addition, subtraction, trigonometic operations, etc.        NMatrix-JRuby is a clear winner when we are working with matrices of arbitrary dimension.  Test Report            Spec file      Total Tests      Success      Failure      Pending                  00_nmatrix_spec      188      139      43      6              01_enum_spec      17      8      09      0              02_slice_spec      144      116      28      0              03_nmatrix_monkeys_spec      12      11      01      0              elementwise_spec      38      21      17      0              homogeneous_spec.rb      07      06      01      0              math_spec      737      541      196      0              shortcuts_spec      81      57      24      0              stat_spec      72      40      32      0              slice_set_spec      6      2      04      0      Why some tests fail?  Complex dtype has not been implemented.  Sparse matrices (list and yale) have not been implemented.  Decomposition methods that are specific to LAPACK and ATLAS have not been implemented.  Integer dtype not properly assigned to Floor, Ceil and Round.ConclusionThe main goal of this project was to bring  Scientific Computation to JRuby, to gain from the performance JRuby offers.By the end of the GSoC, we have been able to successfully create a linear algebra library, NMatrix for JRuby users, which they can easily run on their machines unless they want to use Complex numbers.We have mixed-models gem simultaneously ported to JRuby. Even here, we are very close to MRI if performance is considered.Future workIn the coming months we would be implementing Sparse Matrices, thus making NMatrix a complete package for JRuby users. We would also work on improving performance using parallelization.We also feel that JRuby lacks its own Jupyter notebook. The iruby notebook doesn’t work for JRuby. To create an amazing experience for scientific computation on JRuby, we will be porting iruby to  JRuby.AcknowledgementsI would like to express my sincere gratitude to my mentor Pjotr Prins for the continuous support through the summers, and for his patience, motivation, enthusiasm, and immense knowledge. I could not have imagined having a better advisor and mentor, for this project.I am very grateful to Google and the Ruby Science Foundation for this golden opportunity.I am very thankful to Charles Nutter, John Woods, Sameer Deshmukh, Kenta Murata and Alexej Gossmann, who mentored me through the project. It has been a great learning experience.I thank my fellow GSoC participants Rajith, Lokesh and Gaurav who helped me with certain aspects of my project.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/jruby/2016/08/22/gsoc16-project-report-port-nmatrix-to-jruby.html"
			      }
			      ,
			    
			      "jruby-ruby-gem-2016-07-09-gsoc16-sciruby-mixedmodels-jruby-html": {
			        "title": "JRuby Port of Mixed-Models",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "IntroductionFor my GSoC 2016 project of JRuby port of NMatrix, we worked on testing NMatrix-JRuby with real-life data. We started with mixed-models gem.Mixed models are statistical models which predict the value of a response variable as a result of fixed and random effects. All matrix calculations are performed using the gem nmatrix, which has a quite intuitive syntax and contributes to the overall code readability as well.Mixed ModelsThe real motivation for working with JRuby port of Mixed-Models was to work with real-world data. We ran the code from this blog by Alexej Gossman, the author of ‘mixed-models’ gem which explains using mixed-models with some examples, using JRuby. We then compared the results for Ruby-MRI and JRuby.Example1 : LMMI started running example LMM.rb. Alexej wrote a blog explaining this example and it can be found here. I ran the example using both ruby and jruby and compared output at every stage. Here I found these issues.Rank of a matrixOne of the most important part of NMatrix was indexing. NMatrix stores multi-dimensional arrays as flat arrays and the indexing and slicing of elements is done using the shape, dimension, stride and offset of NMatrix. It would not be justice with NMatrix if I don’t discuss about slicing and enumerators in NMatrix; so in my next blog I will discuss about slicing and also enumerators.While working with LMM, there was an issue with getting the rank of a matrix. The rank couldn’t be recursively accessed as the new matrix returned was not assigned dimension. This was a small bug but too tough to detect it.Cholesky/LUD Decomposition to solve a matrix when constants are a n x p matrixGiven we need to solved a system of linear equations                    AX = Bwhere A is an m×n matrix, B and X are n×p matrices, we needed to solve this equation by iterating through B.Initially, for NMatrix-jruby we considered that X and B are column vectors. So, we got an exception ‘dimension error’. There was a similar issue with NMatrix -MRI : issue.We solved this issue by implementing NMatrix#matrix_solve that is called by method triangular_solve when using JRuby.  def matrix_solve b    if b.shape[1] &gt; 1      nmatrix = NMatrix.new :copy      nmatrix.shape = b.shape      result = []      res = []      (0...b.shape[1]).each do |i|        res &lt;&lt; self.solve(b.col(i)).s.toArray.to_a      end      index = 0      (0...b.shape[0]).each do |i|        (0...b.shape[1]).each do |j|          result[index] = res[j][i]          index+=1        end      end      nmatrix.s = ArrayRealVector.new result.to_java :double      nmatrix.twoDMat =  MatrixUtils.createRealMatrix get_twoDArray(b.shape, result)            return nmatrix    else      return self.solve b    end  endDot productI was stuck at another issue in model_fit. Triangular solve method which calls cholesky to solve linear equations threw singular matrix exception.  I wasn’t unable to figure out what was wrong.  I started comparing the output of LMM.rb using ruby and jruby at each stage. y vector is critical for model fit. Apparently, the elements of y were different in the two cases. Looking closely, I found z.dot b to be returning a matrix with all the elements 0.  This is what was happening:...# Generate the response vectory = (x.dot beta) + (z.dot b) + epsilon# Set up the covariance parametersparametrization = Proc.new do |th|   diag_blocks = Array.new(5) { NMatrix.new([2,2], [th[0],th[1],0,th[2]], dtype: :float64) }  NMatrix.block_diagonal(*diag_blocks, dtype: :float64) end# Fit the modelmodel_fit = LMM.new(x: x, y: y, zt: z.transpose,                    start_point: [1,0,1],                     lower_bound: Array[0,-Float::INFINITY,0],                    &amp;parametrization) ...            When we take dot product of two matrices C = A.dot B. If Aij or Bij are smaller than      1      we get Cij = 0 . So, yeah its autoboxing.      Currently, I solved this by usingy = (x.dot beta) + ((z * 5).dot b)/5 + epsilonThus, we get the value of y vector same for both cases.Cholesky solve throws “singular matrix exception”When LMM does optimisation line [5] it calls NelderMead.minimize that uses deviation; and autoboxing leads to 0 as element output. Therefore a diagonal matrix gets reduced to a singular matrix and Cholesky solve throws “singular matrix” error [6]....model_fit = LMM.new(x: x, y: y, zt: z.transpose,                    start_point: [1,0,1],                     lower_bound: Array[0,-Float::INFINITY,0],                    &amp;parametrization) ...Here z.transpose is wrong due to boxing. So, we usedzt = (z*5).transpose/5model_fit = LMM.new(x: x, y: y, zt: zt,                    start_point: [1,0,1],                     lower_bound: Array[0,-Float::INFINITY,0],                    &amp;parametrization) This error is not always replicated.ResultMIxed-models using Ruby-MRI(1) Model fitOptimal theta:  [4.761283990026765, -0.12007961616262416, 0.5005024020787956]REML criterion:   162.90752516637906(2) Fixed effectsIntercept:  Slope:  (3) Random effectsRandom intercept sd:  3.929341245265398Random slope sd:  0.42437637915866583Correlation of random intercept and slope:  -0.23341842320756737(4) ResidualsVariance:   0.6800937307812478Standard deviantion:  0.824677955799261MIxed-models using JRuby initially gave the following result.(1) Model fitOptimal theta:  [0.0056475944592377265, -5.661316609380864e-05, 0.0]REML criterion:   379.0971583367289(2) Fixed effectsIntercept:  Slope:  (3) Random effectsRandom intercept sd:  0.06041274692971716Random slope sd:  0.0006062864584118943Correlation of random intercept and slope:  -1.0(4) ResidualsVariance:   114.11688631756937Standard deviantion:  10.682550553007898There was an error in NMatrix#matrix_solve. After correcting it, we get the correct result.(1) Model fitOptimal theta:  [4.761283990026765, -0.12007961616262416, 0.5005024020787956]REML criterion:   162.9075251663791(2) Fixed effectsIntercept:  Slope:  (3) Random effectsRandom intercept sd:  3.929341245265402Random slope sd:  0.4243763791586663Correlation of random intercept and slope:  -0.2334184232075674(4) ResidualsVariance:   0.6800937307812492Standard deviantion:  0.8246779557992618Next, we ran other examples and we got the correct results as we expected.Example2 : Blog_dataBlog_data example deals with real data. Initially, mixed_models was not supported by latest daru. This issue has been resolved by Alexej.Currently NMatrix-JRuby has not been optimized. It is not memory efficient. Running blog_data.rb results in OutOfMemoryError even when 12GB of heap-size is alloted to JVM.This is a work in progress.Test Report            Spec file      Total Test      Success      Failure      Pending                  Deviance_spec      04      04      0      0              LMM_spec      195      195      0      0              LMM_categorical_data_spec.rb      48      45      3      0              LMMFormula_spec.rb      05      05      0      0              LMM_interaction_effects_spec.rb      82      82      0      0              LMM_nested_effects_spec.rb      40      40      0      0              matrix_methods_spec.rb      52      48      4      0              ModelSpecification_spec.rb      07      07      0      0              NelderMeadWithConstraints_spec.rb      08      08      0      0      Features not Supported  Parallel Gem: Currently Parallel gem is not supported by JRuby. So, we can’t use parallel processing to utilize multiple CPU cores. /example/bootstrap.rb can’t be run with parallelism.  ArrayStoreException: We are not exactly sure why this occurs currently. We guess it’s due to a lot of memory used by arrays. We believe it can be overcome once we optimize NMatrix-JRuby. This issue was previously reported on JRuby issues page.  Process.fork not supported: JRuby currently doesn’t support fork. So, we had to run some tests individually which failed while running the entire test file.Conclusion:We have successfully ported mixed_models gem to JRuby. All examples (except blog_data.rb and bootstrap.rb) produce correct results. Now we need to optimize the performance of mixed_models gem which will mostly involve optimizing NMatrix-JRuby as blog_example which deals with real data, runs out of memory.Next, we will implement ruby-objects as dtype and clear a few tests still remaining for NMatrix-JRuby.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/jruby/ruby/gem/2016/07/09/gsoc16-sciruby-mixedmodels-jruby.html"
			      }
			      ,
			    
			      "ruby-gem-2016-06-16-gsoc16-sciruby-nmatrix-jruby-html": {
			        "title": "JRuby Port of NMatrix",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "IntroductionNMatrix for MRI has become a fairly well-established project. NMatrix is SciRuby’s numerical matrix core, implementing dense matrices as well as two types of sparse (linked-list-based and Yale/CSR). The backend has been written in C/C++ and relies on ATLAS/ CBLAS / CLAPACK and standard LAPACK for several of its linear algebra operations.With JRuby + Truffle + Graal becoming very fast compared to MRI, a lot of ruby developers are switching to JRuby. JRuby compiles to byte code making it fast and also does heavy optimizations before and after generating byte code. However NMatrix does not run on JRuby because of C-libraries, and as a result they’re not usable on JRuby. The more of these libraries we have ports for, the less pain JRuby users suffer during migration.This project aims to port NMatrix to JRuby. NMatrix being a huge library; we aim to implement NMatrix on Jruby for dense matrices during the summers. Every feature of NMatrix that has been ported to JRuby will be benchmarked versus NMatrix-CRuby.BenchmarkingAfter the first iteration, we benchmarked NMatrix JRuby versus NMatrix CRuby for matrix addition, subtraction and multiplication and got some interesting results.The results were promising but there was a huge scope for improvement. The following sections discuss how we built the new backend and how we optimized it after the first iteration.Creating a new backendThe java backend aims to create a wrapper around Commons Maths. The Apache Commons Mathematics Library is a library of lightweight, self-contained mathematics and statistics components addressing the most common problems not available in the Java programming language or Commons Lang. It has a very good community support. We used Version: 3.6.1.Like any other ruby gem with c extensions, the backend code lies within ext/ directory. The new java backend has been placed in ext/nmatrix_java/ directory.NMatrix Creation for arbitrary dimension:We started with double dense matrices and as soon as double matrices are completely implemented; we will implement other data-types. The JNMatrix class stores the the NMatrix elements. The elements of a matrix are stored as a flat_array using the ArrayRealVector class provided by Commons Math. ArrayRealVector class provides methods for operations like addition, subtraction and multiplication. The function mapToSelf(Univariate function) maps a function to each element that facilitates using different element-wise operations like sin(), cos(), floor(), ceil().When we have matrices of two dimensions, we use JNMatrixTwoD class. Whenever a new JNMatrix object is initialised the constructor checks if it is two dimensional. If true a JNMatrixTwoD object is initialised. Now we can operate on 2-D matrices. JNMatrixTwoD wraps Array2DRowRealMatrix and BlockRealMatrix. They store elements of the matrix in row major format. BlockRealMatrix implementation is specially designed to be cache-friendly.Square blocks are stored as small arrays and allow efficient traversal of data both in row major direction and columns major direction, one block at a time. This greatly increases performances for algorithms that use crossed directions loops like multiplication or transposition. 2D double matrices currently support determinants, inverse, isSymmetric? .Connecting with the new backendThe nmatrix c extension uses lib/nmatrix/nmatrix.rb to connect the backend and frontend. It loads the nmatrix.so file. The shared objects act as a bridge between the c api and ruby frontend code. For java extensions; we created a lib/nmatrix/nmatrix_java.rb file that gets loaded only when java is detected as ruby platform.lib/nmatrix/nmatrix.rbdef jruby?  /java/ === RUBY_PLATFORMendif jruby?    require 'nmatix_java.rb'else    require \"nmatrix.so\"endlib/nmatrix/nmatrix_java.rbrequire 'java'require_relative '../../ext/nmatrix_java/vendor/commons-math3-3.6.1.jar'require_relative '../../ext/nmatrix_java/target/nmatrix.jar'The lib/nmatrix/nmatrix_java.rb file further creates a NMatrix class and binds it with java backend apis. This file is analogous to ruby_nmatrix.c where NMatrix class is defined along with the methods.NMatrix JRuby doesn’t seem to be RAM friendly as it consumes a lot of RAM as compared to NMatrix-CRuby.For 6GB RAM we can multiply 5000 x 5000 matrix with a 5000 x 5000 RAM.To multiply a 6,000 x 6,000 matrix with a 6,000*6,000 matrix we need 10gb ram.This is evident that error scales up as we scale up the size. For example initialization of a 6000 by 60000 matrix  takes 9.89 to 13 seconds.For a RealMatrix interface, the type of matrix returned depends on the dimension. Below 2^12 elements (i.e. 4096 elements or 64*64 for a square matrix) which can be stored in a 32kB array,  a Array2DRowRealMatrix instance is built. Above this threshold a BlockRealMatrix instance is built.Block SizeIn java applications, data is read from and written to disk in units known as blocks. The Block Size property specifies the number of bytes per block.According to Commons Math a BlockRealMatrix is a“Cache-friendly implementation of RealMatrix using a flat arrays to store square blocks of the matrix.This implementation is specially designed to be cache-friendly. Square blocks are stored as small arrays and allow efficient traversal of data both in row major direction and columns major direction, one block at a time. This greatly increases performances for algorithms that use crossed directions loops like multiplication or transposition.The size of square blocks is a static parameter. It may be tuned according to the cache size of the target computer processor. As a rule of thumbs, it should be the largest value that allows three blocks to be simultaneously cached (this is necessary for example for matrix multiplication). The default value is to use 52x52 blocks which is well suited for processors with 64k L1 cache (one block holds 2704 values or 21632 bytes). This value could be lowered to 36x36 for processors with 32k L1 cache.The regular blocks represent BLOCK_SIZE x BLOCK_SIZE squares. Blocks at right hand side and bottom side which may be smaller to fit matrix dimensions. The square blocks are flattened in row major order in single dimension arrays which are therefore BLOCK_SIZE2 elements long for regular blocks. The blocks are themselves organized in row major order.As an example, for a block size of 52x52, a 100x60 matrix would be stored in 4 blocks. Block 0 would be a double[2704] array holding the upper left 52x52 square, block 1 would be a double[416] array holding the upper right 52x8 rectangle, block 2 would be a double[2496] array holding the lower left 48x52 rectangle and block 3 would be a double[384] array holding the lower right 48x8 rectangle.The layout complexity overhead versus simple mapping of matrices to java arrays is negligible for small matrices (about 1%). The gain from cache efficiency leads to up to 3-fold improvements for matrices of moderate to large size.”Initialialization of a matrix in NMatrix-jrubylib/nmatrix/nmatrix_java.rbdef initialize  @shape = [shape,shape] unless shape.is_a?(Array)  @s = elements  @nmat= JNMatrix.new(@shape, @elements , \"FLOAT32\", \"DENSE_STORE\" )endext/nmatrix_java/nmatrix/JNMatrix.javapublic JNMatrixTwoD(int[] shape, double[] oneDArray){    set_rows(shape[0]);    set_cols(shape[1]);    this.nmat2d = MatrixUtils.createRealMatrix(this.two_d_array_generator(shape,         oneDArray));    this.nmat2dblock = new BlockRealMatrix(this.two_d_array_generator(shape,  oneDArray));    if (shape[0] == shape[1]){      solver = new LUDecomposition(this.nmat2d);    }  }ext/nmatrix_java/nmatrix/JNMatrixTwoD.javapublic static RealMatrix createRealMatrix(final int rows, final int columns) {        return (rows * columns &lt;= 4096) ?                new Array2DRowRealMatrix(rows, columns) : new BlockRealMatrix(rows, columns);    }lib/nmatrix/nmatrix_java.rb ( Matrix Addition )def +(other)  result = nil  if (other.is_a?(NMatrix))    #check dimension    #check shape    if (@dim != other.dim)      raise Exception.new(\"cannot add matrices with different dimension\")    end    #check shape  (0...dim).each do |i|    if (@shape[i] != other.shape[i])      raise Exception.new(\"cannot add matrices with different shapes\");    end  end  resultArray = @nmat.add(other.nmat).to_a  result = NMatrix.new(shape, resultArray,  dtype: :int64)  else  resultArray = @nmat.mapAddToSelf(other).to_a  result = NMatrix.new(shape, resultArray,  dtype: :int64)  end  resultendlib/nmatrix/nmatrix_java.rb ( Matrix Multiplication )def dot(other)  result = nil  if (other.is_a?(NMatrix))    #check dimension    #check shape    if (@shape.length!=2 || other.shape.length!=2)      raise Exception.new(\"please convert array to nx1 or 1xn NMatrix first\")      return nil    end    if (@shape[1] != other.shape[0])      raise Exception.new(\"incompatible dimensions\")      return nil    end    resultArray = @nmat.twoDMat.multiply(other.nmat.twoDMat).to_a    newShape= [@shape[0],other.shape[1]]    result = NMatrix.new(newShape, resultArray,  dtype: :int64)  else    raise Exception.new(\"cannot have dot product with a scalar\");  end  return result;endext/nmatrix_java/nmatrix/JNMatrixTwoD.javapublic double[] multiply(JNMatrixTwoD other){  RealMatrix result = this.nmat2d.multiply(other.nmat2d);  return this.one_d_array_generator(rows, cols, result.getData());}Lets take a look at what happens when we benchmark matrix multiplication.shapeArray = [              [10,10],[50,50],                      // array2dRowRealMatrix              [100,100],[500,500],               [1000,1000],[2000,2000],[3000,3000],  // BlockRealMatrix              [4000,4000],[5000,5000],            ]For shape = [5000,5000] we generate an array of random elements.RAM size calculation for storing a single matrix of 5,000 x 5,000 elements.      5000*5000 =&gt; 5000/52 x 5000/52   =&gt; 97x97 blocks x 21.632KB (Since,1 block of 2704 elements takes 21.362KB space.)   =&gt; 203,535KB=&gt;203MB=&gt; 0.2GB    Real array=&gt;  Three arrays of shape 5,000 x 5,000 =&gt;0.2 x 3 = 0.6GB.    Multiplication =&gt; Three matrices of shape 5,000 x 5,000 =&gt;0.2 x 3 = 0.6GB.In the process, while using multiplication api, commons math creates its own copy which consumes 0.2gb more.Memory required is at least 1.6gbThree initializations =&gt; 3 x 0.6GB + 0.6GB =&gt; 2.4GB &lt;= java copyRuby copy would be storing the elements =&gt; 0.6GB }&lt;= ruby copyThis calculation thus requires 3 GB.Pass by Value and Pass by referenceIn the current code ,The input array is copied, not referenced at a lot of places. This consumes a lot of memory, upsets the Garbage Collector and slows down the program.Solution  Minimise initializations  Don’t copy again and againWe need to work only in terms of apis provided by Commons-Math.jarAfter 2nd IterationWe benchmarked the code after a few improvements. These are the new graphs that we obtained.Instead of using a separate java class to store NMatrix element we used it directly in nmatrix_java.rb.  Now we don’t load nmatrix.jar.lib/nmatrix/nmatrix_java.rbdef initialize(shape, elements)  @shape = [shape,shape] unless shape.is_a?(Array)  @s = ArrayRealVector(elements)  if shape.length == 2    @twoDMat = get_twoDMat(shape,elements)  endendNow there are just two initializations (only 1 if we don’t have 2D Matrix). Also, there is less “passing by value” to functions.lib/nmatrix/nmatrix_java.rb ( Matrix Addition )def +(other)  result = NMatrix.new(:copy)  result.shape = @shape  if (other.is_a?(NMatrix))    #check dimension    #check shape    if (@dim != other.dim)      raise Exception.new(\"cannot add matrices with different dimension\")    end    #check shape    (0...dim).each do |i|      if (@shape[i] != other.shape[i])        raise Exception.new(\"cannot add matrices with different shapes\");      end    end    result.s = @s.add(other.s)  else    result.s = @s.mapAddToSelf(other)  end  resultendlib/nmatrix/nmatrix_java.rb ( Matrix Multiplication )def dot(other)  result = nil  if (other.is_a?(NMatrix))    #check dimension    if (@shape.length!=2 || other.shape.length!=2)      raise Exception.new(\"please convert array to nx1 or 1xn NMatrix first\")      return nil    end    #check shape    if (@shape[1] != other.shape[0])      raise Exception.new(\"incompatible dimensions\")      return nil    end        result = NMatrix.new(:copy)    result.shape = @shape    result.twoDMat = @twoDMat.multiply(other.twoDMat)    result.s = ArrayRealVector.new(get_oneDArray(@shape, result.twoDMat.getData()))  else    raise Exception.new(\"cannot have dot product with a scalar\");  end  return result;endWe generate minimum number of copies. In binary and unary operations, the resultant matrix is initialized as a blank nmatrix. We then just point the result of the operation to the storage. So, the JRuby virtual machine doesn’t have to create new copies and the Garbage collector is not upset at all. Thus we see, a great deal of performance boost.AutoboxingAlso passing values means coercing them in the required format. Now we don’t have to worry a lot about coercion of values.ResultsFrom the above graphs, we see that for addition and subtraction, NMatrix- JRuby is the clear winner.NMatrix- Lapacke is the clear winner in matrix multiplication. NMatrix-Jruby competes closely with NMatrix-MRI. We can still optimize it to perform better, especially matrix multiplication.TestsWe used the existing tests for NMatrix-MRI for the development. The program detects on runtime which method to load.The table given below summarises how many tests succeed currentlyusing NMatrix-jruby.            Spec file      Total Test      Success      Failure      Pending                  00_nmatrix_spec      188      80      102      6              02_slice_spec      144      20      120                     03_nmatrix_monkeys_spec      12      4      8                     elementwise_spec      38      4      34                     math_spec      737      110      598                     shortcuts_spec      81      21      60                     stat_spec      72      28      54             Improvements and future workImplement solvers for two dimensional matrices and enumerators by the end of mid term and parallely optimize these features.After mid-term evaluations, we will be implementing complex dtype using FieldRealVector and FieldMatrix followed by other data-types.Please enable JavaScript to view the comments powered by Disqus.",
			        "url": "/ruby/gem/2016/06/16/gsoc16-sciruby-nmatrix-jruby.html"
			      }
			      ,
			    
			      "ruby-gem-2016-03-30-mitab-html": {
			        "title": "mitab (MITab file parser)",
			        "author": "Prasun Anand",
			        "category": "",
			        "content": "I built a ruby-gem “mitab” that helps in parsing a PSI-Mitab file.todo:I have been working on “Port NMatrix to JRuby” as my GSoC project. I am pleased to announce that JRuby is ready for Nmatrix users.NMatrix, a linear algebra library wraps Apache Commons Maths for its core functionalities. By the end of GSoC, I have been able to implement NMatrix for dense matrices with double and object ( ruby objects ) data type. I have also worked on porting mixed-models gem to JRuby which heavily uses NMatrix at its core.Installing the gem:$ gem install mitabWorking with the gem:require 'mitab'text = open(filename) { |f| f.read }m = Mitab::MitabParser.new(text)m.printputs m.mitabputs m.nodesputs m.scoresputs m.linksCheck out the Mitab Gem. File all bugs/feature requests at Mitab’s GitHub repo. If you have questions, you can mail them to prasunanand.bitsp@gmail.comPlease enable JavaScript to view the comments powered by Disqus.",
			        "url": "/ruby/gem/2016/03/30/mitab.html"
			      }
			      
			    
			  };
			</script>
			<script src="/js/lunr.min.js"></script>
			<script src="/js/search.js"></script>

          </div>


        </div>
      </div>
    </section>


    <section id="updates">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <div class="updates-wraper">
              <div class="updates-image wow fadeInUp" data-wow-delay="0.2s">
                <img src="/images/image2.svg" alt="#">
              </div>
              <div class="updates-form header-form wow fadeInUp" data-wow-delay="0.2s">
                <h5 class="font-h5">Subscribe for exclusive</h5>
                <h3 class="font-h3">news and updates!</h3>
                <form action="https://zasper.io/newsletter" method="POST">
                  <div class="form-group">
                    <input type="email" name="email" placeholder="Enter your email">
                    <button type="submit" class="btn-bg hover-bg">Subscribe</button>
                  </div>
                </form>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section id="footer">
      <div class="container">
        <div class="row">
          <div class="col-12 col-md-4">
            <div class="footer-info wow fadeInUp" data-wow-delay="0.2s">
              <div class="footer-image wow fadeInUp" data-wow-delay="0.1s">
                <img src="/images/logo.svg" alt="#">
              </div>
              <div class="footer-content wow fadeInUp" data-wow-delay="0.2s">
                <p>Zasper strives to be at the forefront of the AI Revolution that is taking place around the world. We want to ease the pain of Data teams by enabling them to develop and deploy their AI solutions right from rapid prototyping to shipping into production.</p>
              </div>
              <div class="footer-social wow fadeInUp" data-wow-delay="0.3s">
                <a href="https://www.linkedin.com/company/zasper"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
                <a href="#"><i class="fa fa-twitter" aria-hidden="true"></i></a>
                <a href="https://github.com/zasperio"><i class="fa fa-github" aria-hidden="true"></i></a>
              </div>
            </div>
          </div>
          <div class="col-12 col-md-8">
            <div class="footer-menu-wraper wow fadeInUp" data-wow-delay="0.4s">
              <div>
                <div class="footer-menu wow fadeInUp" data-wow-delay="0.1s">
                  <h6>Company</h6>
                  <a href="https://zasper.io/product">Product</a>
                  <a href="https://zasper.io/pricing">Pricing</a>
                  <a href="https://zasper.io/partners">Partners</a>
                  <a href="https://zasper.io/careers">Careers</a>
                  <a href="https://zasper.io/support">Support</a>
                </div>
              </div>
              <div>
                <div class="footer-menu wow fadeInUp" data-wow-delay="0.3s">
                  <h6>Solutions</h6>
                  <a href="https://zasper.io/solutions/banking">Banking</a>
                  <a href="https://zasper.io/solutions/telecom">Telecom</a>
                  <a href="https://zasper.io/solutions/oil-and-gas">Oil and Gas</a>
                  <a href="https://zasper.io/solutions/healthcare">Healthcare</a>
                  <a href="https://zasper.io/solutions/">View More</a>
                </div>
              </div>
              <div>
                <div class="footer-menu wow fadeInUp" data-wow-delay="0.5s">
                  <h6>Community</h6>
                  <a href="/">Blog</a>
                  <a href="https://docs.zasper.io">Docs</a>
                  <a href="https://zasper.io/contact-us">Contact Us</a>
                </div>
              </div>
              <div>
                <div class="footer-menu wow fadeInUp" data-wow-delay="0.6s">
                  <h6>Legal</h6>
                  <a href="https://zasper.io/terms-and-conditions">Terms and Conditions</a>
                  <a href="https://zasper.io/privacy">Privacy Policy</a>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section id="copyright">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <div class="copyright-text wow fadeInUp" data-wow-delay="0.2s">
              <p>©2021 zasper ai labs pvt ltd | All rights reserved.</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- JavaScripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/wow.min.js"></script>
    <script src="/js/owl.carousel.min.js"></script>
    <script src="/js/main.js"></script>

  </body>
</html>




